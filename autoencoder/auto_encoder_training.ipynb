{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from utils import extract_full_vector, extract_subset_vector, extract_subset_vector_exp_lip, unflatten_vector, map_subset_to_full_vector, extract_subset_vector_xs_lip\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkls/dataset_descriptor/live_portrait_descriptor_all_with_mead.pkl', 'rb') as file:\n",
    "    all_descriptors = pickle.load(file) # frame name is the key and the value is the descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M009/front/happy/level_3/017/frame_0113.jpg\n",
      "{'c_d_eyes_lst': [array([[0.32350767, 0.3143503 ]], dtype=float32)], 'c_d_lip_lst': [array([[0.28211427]], dtype=float32)], 'driving_template_dct': {'n_frames': 1, 'output_fps': 25, 'motion': [{'scale': array([[1.3642578]], dtype=float32), 'R': array([[[ 0.9988814 ,  0.01929191,  0.0431716 ],\n",
      "        [-0.01519142,  0.9955155 , -0.09337095],\n",
      "        [-0.0447793 ,  0.09261066,  0.99469495]]], dtype=float32), 'exp': array([[[ 4.0779114e-03,  1.4381409e-02,  1.6689301e-05],\n",
      "        [-3.6792755e-03,  2.2468567e-03,  1.1324883e-06],\n",
      "        [ 3.1051636e-03, -4.1580200e-03,  2.0861626e-06],\n",
      "        [-2.2184849e-04,  3.9749146e-03,  1.5616417e-05],\n",
      "        [ 2.3210049e-04,  7.8430176e-03, -1.0013580e-05],\n",
      "        [-6.9093704e-04, -1.4209747e-03, -3.8146973e-06],\n",
      "        [ 2.7358532e-05,  5.3584576e-05,  2.3841858e-07],\n",
      "        [ 1.1243820e-03, -4.9743652e-03, -2.8610229e-06],\n",
      "        [-1.2218952e-05,  2.4080276e-05, -6.5565109e-07],\n",
      "        [ 4.5299530e-06, -8.4638596e-06,  3.5762787e-07],\n",
      "        [ 7.3699951e-03,  4.9686432e-04,  6.1454773e-03],\n",
      "        [-6.0033798e-04, -9.1552734e-04,  1.9073486e-06],\n",
      "        [-2.6822090e-06,  5.2452087e-06,  5.3644180e-07],\n",
      "        [ 1.1825562e-03, -1.2226105e-03,  1.3113022e-06],\n",
      "        [-3.5786629e-04, -3.8757324e-03, -5.9604645e-07],\n",
      "        [ 4.3320656e-04, -1.2865067e-03, -2.9802322e-07],\n",
      "        [-1.9292831e-03, -1.3132095e-03, -9.5367432e-06],\n",
      "        [-6.5565109e-04,  1.4753342e-03, -1.3220310e-04],\n",
      "        [-1.8954277e-05,  3.2603741e-05, -8.3446503e-07],\n",
      "        [-6.8321228e-03, -1.9592285e-02,  1.6021729e-03],\n",
      "        [-3.8146973e-04, -1.5914917e-02, -6.0691833e-03]]], dtype=float32), 't': array([[-0.01351929,  0.05682373,  0.        ]], dtype=float32), 'kp': array([[[ 0.07385254, -0.3930664 ,  0.14453125],\n",
      "        [-0.30029297, -0.33642578, -0.17443848],\n",
      "        [ 0.16479492, -0.35009766, -0.26953125],\n",
      "        [-0.28588867,  0.18725586,  0.08563232],\n",
      "        [ 0.3347168 ,  0.06213379,  0.19763184],\n",
      "        [ 0.01647949,  0.19372559,  0.26538086],\n",
      "        [-0.3659668 ,  0.10949707,  0.81347656],\n",
      "        [ 0.32202148,  0.3088379 , -0.01486206],\n",
      "        [-0.39160156, -0.05352783,  0.1505127 ],\n",
      "        [ 0.31176758, -0.2019043 ,  0.11688232],\n",
      "        [-0.13964844, -0.08862305,  0.1862793 ],\n",
      "        [-0.13232422, -0.11633301, -0.13586426],\n",
      "        [-0.13354492, -0.10076904, -0.13977051],\n",
      "        [-0.13549805, -0.07739258, -0.1303711 ],\n",
      "        [ 0.16333008, -0.11767578, -0.1385498 ],\n",
      "        [ 0.16149902, -0.10144043, -0.13439941],\n",
      "        [ 0.16418457, -0.07995605, -0.13342285],\n",
      "        [-0.04119873,  0.19580078, -0.17858887],\n",
      "        [ 0.12133789,  0.22033691, -0.14233398],\n",
      "        [ 0.01081848,  0.23205566, -0.19091797],\n",
      "        [ 0.01012421,  0.2088623 , -0.21325684]]], dtype=float32), 'x_s': array([[[ 0.09200206, -0.43719095,  0.25057393],\n",
      "        [-0.41012856, -0.42696708, -0.21154682],\n",
      "        [ 0.2390098 , -0.45404708, -0.31145445],\n",
      "        [-0.4125237 ,  0.31986105,  0.07553504],\n",
      "        [ 0.4295654 ,  0.18568923,  0.27997592],\n",
      "        [-0.012232  ,  0.35195482,  0.33641544],\n",
      "        [-0.56416154,  0.29875576,  1.0684022 ],\n",
      "        [ 0.4213508 ,  0.47608003, -0.04054626],\n",
      "        [-0.5552694 , -0.00713182,  0.18800214],\n",
      "        [ 0.40838656, -0.19442916,  0.2026934 ],\n",
      "        [-0.20331128, -0.04300091,  0.2642329 ],\n",
      "        [-0.18394971, -0.12307018, -0.17734276],\n",
      "        [-0.18488175, -0.10120164, -0.18469995],\n",
      "        [-0.18698505, -0.06999208, -0.17503648],\n",
      "        [ 0.21947047, -0.12149028, -0.16340625],\n",
      "        [ 0.21746445, -0.09543183, -0.15994962],\n",
      "        [ 0.21739614, -0.06609542, -0.16121556],\n",
      "        [-0.06370447,  0.30111322, -0.26989707],\n",
      "        [ 0.15593462,  0.3413269 , -0.21407175],\n",
      "        [-0.00124346,  0.32142207, -0.28581634],\n",
      "        [ 0.00845616,  0.29209846, -0.3236828 ]]], dtype=float32)}], 'c_eyes_lst': [array([[0.32350767, 0.3143503 ]], dtype=float32)], 'c_lip_lst': [array([[0.28211427]], dtype=float32)]}}\n"
     ]
    }
   ],
   "source": [
    "for key in all_descriptors:\n",
    "    print(key)\n",
    "    print(all_descriptors[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting vectors for clustering: 100%|██████████| 9282/9282 [03:51<00:00, 40.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store video frames\n",
    "video_dict = defaultdict(list)\n",
    "\n",
    "# Populate the video_dict with frame arrays in order\n",
    "for key, value in all_descriptors.items():\n",
    "    parts = key.split('/')\n",
    "    if key[0] == \"M\": ## mead dataset\n",
    "        video_name = \"/\".join(parts[:-1])\n",
    "        frame_number = parts[-1].split('.')[0].split(\"_\")[-1]\n",
    "    else:\n",
    "        video_name = parts[1] #rawdes\n",
    "        frame_number = parts[-1].split('.')[0]\n",
    "    video_dict[video_name].append((frame_number, value))\n",
    "\n",
    "# Sort video_dict by keys\n",
    "video_dict = dict(sorted(video_dict.items()))\n",
    "\n",
    "# Sort each video's frames by frame number\n",
    "for video_name in video_dict:\n",
    "    video_dict[video_name].sort(key=lambda x: int(x[0]))\n",
    "\n",
    "all_vectors_full = []\n",
    "for video_name, frames in tqdm(video_dict.items(), desc=\"Extracting vectors for clustering\"):\n",
    "    video_frame_vectors = []\n",
    "    for frame_number, frame_data in frames:\n",
    "        vector_full = extract_subset_vector(extract_full_vector(frame_data))\n",
    "        all_vectors_full.append(vector_full)\n",
    "all_vectors_full = np.vstack(all_vectors_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "for key in video_dict:\n",
    "    parts = key.split('/')\n",
    "    if key[0] != \"M\": ## mead dataset\n",
    "        for k in video_dict[key]:\n",
    "            print(k[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "# Split into train (90%) and temp (10%)\n",
    "train_data, temp_data = train_test_split(all_vectors_full, test_size=0.1, random_state=42)\n",
    "\n",
    "# Split temp into validation (5%) and test (5%)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)  # Input and target are the same\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "test_dataset = TensorDataset(test_tensor, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to build a custom MLP (Multi-Layer Perceptron)\n",
    "def build_mlp(layers, activation_functions):\n",
    "    modules = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        modules.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        if activation_functions[i] is not None:\n",
    "            modules.append(activation_functions[i]())\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "# Encoder Module\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, layer_dims, activations):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = build_mlp([input_dim] + layer_dims, activations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Decoder Module\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, layer_dims, activations):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = build_mlp([latent_dim] + layer_dims, activations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Autoencoder combining Encoder and Decoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations\n",
    "input_dim = 72\n",
    "encoded_dim = 16\n",
    "encoder_layers = [input_dim, 512, 256, encoded_dim]  # Number of neurons in each encoder layer\n",
    "encoder_activations = [nn.ReLU, nn.ReLU, nn.ReLU, nn.ReLU]  # Activation functions for encoder\n",
    "decoder_layers = [256, 512, input_dim]  # Number of neurons in each decoder layer\n",
    "decoder_activations = [nn.ReLU, nn.ReLU, None]  # Activation functions for decoder\n",
    "latent_dim = encoder_layers[-1]  # Latent dimension is the output of the last encoder layer\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = Encoder(input_dim=input_dim, layer_dims=encoder_layers, activations=encoder_activations)\n",
    "decoder = Decoder(latent_dim=latent_dim, layer_dims=decoder_layers, activations=decoder_activations)\n",
    "\n",
    "# Combine into autoencoder\n",
    "autoencoder = Autoencoder(encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 0.0172, Val Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50] - Train Loss: 0.0007, Val Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50] - Train Loss: 0.0004, Val Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50] - Train Loss: 0.0003, Val Loss: 0.0001\n",
      "Validation loss did not improve for 1 consecutive epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] - Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Validation loss did not improve for 2 consecutive epochs.\n",
      "Early stopping triggered at epoch 5. Best Val Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "threshold = 1e-4  # Minimum change in validation loss to consider as an improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    autoencoder.train()\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False)\n",
    "\n",
    "    for inputs, _ in train_progress:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update cumulative average loss\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        train_progress.set_postfix({\"Train Loss\": avg_train_loss})\n",
    "\n",
    "    # Validation Phase\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "    val_progress = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_progress:\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            # Update cumulative average loss\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "            avg_val_loss = val_loss / val_steps\n",
    "            val_progress.set_postfix({\"Val Loss\": avg_val_loss})\n",
    "\n",
    "    # Print final losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss - threshold:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve for {epochs_no_improve} consecutive epochs.\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0003076470\n"
     ]
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.10f}\")\n",
    "# Epoch [4/50] - Train Loss: 0.0014, Val Loss: 0.0008 ## 20 ## Test Loss: 0.0012\n",
    "# Epoch [7/50] - Train Loss: 0.0010, Val Loss: 0.0008 ## 16 ## Test Loss: 0.0008\n",
    "# Epoch [6/50] - Train Loss: 0.0011, Val Loss: 0.0008 ## 72 ## Test Loss: 0.0008\n",
    "\n",
    "#latest\n",
    "#Epoch [6/50] - Train Loss: 0.0003, Val Loss: 0.0002 ## Test loss: 0.0002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Vectors Shape: (1156143, 16)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "input_tensor = torch.tensor(all_vectors_full, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 512\n",
    "dataset = TensorDataset(input_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Ensure the encoder is in evaluation mode\n",
    "autoencoder.encoder.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder.encoder.to(device)\n",
    "\n",
    "# Container for encoded vectors\n",
    "encoded_vectors_list = []\n",
    "\n",
    "# Process the input data in batches\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        batch_inputs = batch[0].to(device)  # Get inputs and move to device\n",
    "        encoded_batch = autoencoder.encoder(batch_inputs)  # Encode the batch\n",
    "        encoded_vectors_list.append(encoded_batch.cpu())  # Move to CPU and store\n",
    "\n",
    "# Concatenate all batches into a single tensor\n",
    "encoded_vectors = torch.cat(encoded_vectors_list, dim=0)\n",
    "\n",
    "# Convert back to NumPy array (if needed)\n",
    "encoded_vectors_numpy = encoded_vectors.numpy()\n",
    "\n",
    "# Print the resulting 20-dimensional vectors\n",
    "print(\"Encoded Vectors Shape:\", encoded_vectors_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the encoded vectors (NumPy array) to a pickle file\n",
    "# output_file = \"auto_encoder_vectors.pkl\"\n",
    "\n",
    "# with open(output_file, \"wb\") as f:\n",
    "#     pickle.dump(encoded_vectors_numpy, f)\n",
    "\n",
    "# print(f\"Encoded vectors saved to {output_file}\")\n",
    "\n",
    "def map_subset_to_full_vector_exp_lip(full_vector, subset_output):\n",
    "    c_lip = subset_output[:1]\n",
    "    exp = subset_output[1:]\n",
    "    full_vector[5:6] = c_lip\n",
    "    full_vector[13:76] = exp\n",
    "    return full_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder= autoencoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = dict()\n",
    "dict_encoder_descriptors = dict()\n",
    "for key, value in all_descriptors.items():\n",
    "    full_vector = extract_full_vector(all_descriptors[key])\n",
    "    subset_vector = extract_subset_vector(full_vector)\n",
    "    torch_subset_vector = torch.tensor(subset_vector, dtype=torch.float32).cuda()\n",
    "    encoder_output= autoencoder.encoder(torch_subset_vector)\n",
    "    decoder_output = autoencoder.decoder(encoder_output).cpu().detach().numpy()\n",
    "\n",
    "    encoder_output_numpy = autoencoder.encoder(torch_subset_vector).cpu().detach().numpy()\n",
    "    new_dict[key] = encoder_output_numpy\n",
    "\n",
    "    full_vector = map_subset_to_full_vector(full_vector=full_vector, subset_output=decoder_output)\n",
    "    dict_encoder_descriptors[key] = unflatten_vector(full_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded vectors saved to pkls/auto_encoder_output/encoded_live_portrait_descriptor_all_with_mead.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the encoded vectors (NumPy array) to a pickle file\n",
    "output_file = \"pkls/auto_encoder_output/encoded_live_portrait_descriptor_all_with_mead.pkl\"\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(new_dict, f)\n",
    "\n",
    "print(f\"Encoded vectors saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded vectors saved to pkls/auto_encoder_output/autodecoded_descriptors_live_portrait_descriptor_all_with_mead.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the encoded vectors (NumPy array) to a pickle file\n",
    "output_file = \"pkls/auto_encoder_output/autodecoded_descriptors_live_portrait_descriptor_all_with_mead.pkl\"\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(dict_encoder_descriptors, f)\n",
    "\n",
    "print(f\"Encoded vectors saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder saved successfully to trained_models/encoder_16_with_mead_5_148.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# File path to save the encoder\n",
    "encoder_save_path = \"trained_models/encoder_16_with_mead_5_148.pth\"\n",
    "\n",
    "# Save the encoder's state dictionary\n",
    "torch.save(autoencoder.encoder.state_dict(), encoder_save_path)\n",
    "\n",
    "print(f\"Encoder saved successfully to {encoder_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed2aa2b8f5a84f5c3f552f0a432be96474f62544824e39672ea3e4e332a4d1d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('hface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
