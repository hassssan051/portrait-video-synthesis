{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB0/2k7kMiXzSH8oy7Na90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassssan051/portrait-video-synthesis/blob/audio-to-descriptor-pred/prediction/KNN_FirstFramePredictor_MFCC_Cluster_rep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLnw1GTCdD12",
        "outputId": "6aa823c3-cc24-4a52-a3a2-c9ab30e74b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from collections import defaultdict\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "C7VMgTpUdKoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading MFCC features of RAVDESS dataset\n",
        "datasetPath = 'DatasetForLSTM'\n",
        "Zipped_inside_folder = 'RAVDESS_MFCC'\n",
        "with zipfile.ZipFile('drive/MyDrive/'+Zipped_inside_folder+'.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(datasetPath)\n"
      ],
      "metadata": {
        "id": "lh9GUQJvdSHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#getting descriptors of frames of each video and storing this information in a dictionar (true_descriptors) where key is video name and value is a list of descriptors of its frames.\n",
        "\n",
        "\n",
        "clusters_info = 'Sawaiz_2/pkl_for_lstm_encoded/17_53_50_800'\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/live_portrait_descriptors_all_encoder.pkl'\n",
        "\n",
        "# Open the file in binary read mode and load the data\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "video_dict = defaultdict(list)\n",
        "\n",
        "\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in data.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    video_dict[video_name].append((frame_number, value))\n",
        "\n",
        "\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "final_video_dict = {}\n",
        "for video_name, frames in video_dict.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [frame_data for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    final_video_dict[video_name] = np.vstack(sorted_arrays)\n",
        "true_descriptors = final_video_dict\n",
        "videos_list = list(true_descriptors.keys())\n",
        "print(len(videos_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5_eTafFdSZQ",
        "outputId": "73e621f5-3d87-478d-f8e9-eadd716aba62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Hierarchical Clustering"
      ],
      "metadata": {
        "id": "7KvmSp5MdYUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/cluster_rep_level1.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/frame_to_cluster_level1.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data_raw = pickle.load(file)\n",
        "\n",
        "cluster_level = 4\n",
        "frames_to_clusters_indices = {}\n",
        "clusters_indices= {}\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in frames_data_raw.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    if video_name not in frames_to_clusters_indices:\n",
        "      frames_to_clusters_indices[video_name] = []\n",
        "    try:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "    except:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "\n",
        "\n",
        "\n",
        "clusters_descriptors = []\n",
        "idx = 0\n",
        "for key, val in clusters_indices.items():\n",
        "  clusters_indices[key] = idx\n",
        "  #print(key)\n",
        "  clusters_descriptors.append(clusters_data[key])\n",
        "  idx+=1\n",
        "clusters_descriptors = np.vstack(clusters_descriptors)\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "frames_data = {}\n",
        "for video_name, frames in frames_to_clusters_indices.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [clusters_indices[frame_data] for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    frames_data[video_name] = sorted_arrays\n",
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames.\n",
        "\n",
        "#Here, key is a video name and value is a list of cluster representatives of those clusters to which its frames are mapped\n",
        "\n",
        "clusters_rep_as_ground_truth_for_a_video = {}\n",
        "for video, frames in frames_data.items():\n",
        "  stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "  clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ],
      "metadata": {
        "id": "NH8DM2sTdUFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mead_mfcc_path = \"/content/drive/MyDrive/MEAD_MFCC/\"\n",
        "# Load and prepare data\n",
        "dataset_path = \"DatasetForLSTM/\"+ Zipped_inside_folder\n",
        "\n",
        "\n",
        "#Getting MFCC features of all frames in a cluster and taking their mean\n",
        "cluster_mfcc_rep = {}\n",
        "for key, val in clusters_indices.items():\n",
        "  cluster_mfcc_rep[val] =[]\n",
        "\n",
        "for video, reps in frames_data.items():\n",
        "  if 'M' in video:\n",
        "    video_path = mead_mfcc_path+video+\".csv\"\n",
        "  else:\n",
        "    video_path = dataset_path+\"/\"+video+\".csv\"\n",
        "  df = pd.read_csv(video_path, header=None)\n",
        "  features = df.iloc[:, :].values.astype(np.float32)\n",
        "  for frame_number, cluster_rep in enumerate(reps):\n",
        "    try:\n",
        "      cluster_mfcc_rep[cluster_rep].append(features[frame_number])\n",
        "    except:\n",
        "      continue\n",
        "for key, val in cluster_mfcc_rep.items():\n",
        "  np_array = np.vstack(val)\n",
        "  cluster_mfcc_rep[key] = np.mean(np_array, axis = 0)"
      ],
      "metadata": {
        "id": "TQIEG-0pdd-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 3\n",
        "clusters_labels_ = list(cluster_mfcc_rep.keys())  # Labels (keys)\n",
        "vectors = np.array(list(cluster_mfcc_rep.values()))  # 28-dimensional vectors (values)\n",
        "\n",
        "# Train the KNN model\n",
        "knn = NearestNeighbors(n_neighbors=K, metric='euclidean')  # K=3 for example\n",
        "knn.fit(vectors)\n",
        "\n",
        "def KNNPrediction(test_vector, knn, labels, cluster_mfcc_rep):\n",
        "  distances, indices = knn.kneighbors(test_vector)\n",
        "  closest_neighbors = [labels[idx] for idx in indices.flatten()]\n",
        "  closest_neighbors_mfccs = [cluster_mfcc_rep[idx] for idx in indices.flatten()]\n",
        "  return closest_neighbors, closest_neighbors_mfccs\n"
      ],
      "metadata": {
        "id": "EDkraRRQdoz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_labels(outputs, cluster_descriptors, device):\n",
        "    # Ensure cluster_descriptors is a torch tensor and move to device\n",
        "    cluster_descriptors = torch.tensor(cluster_descriptors, device=device,  dtype=torch.float32)\n",
        "    #print(cluster_descriptors.shape)\n",
        "    # Initialize an empty list to store predicted labels for each frame\n",
        "    predicted_labels = []\n",
        "    #outputs = outputs.squeeze(0)\n",
        "    # Iterate over each frame descriptor in outputs\n",
        "    for frame_descriptor in outputs:\n",
        "        # Calculate Euclidean distances between the frame descriptor and each cluster descriptor\n",
        "        #print(frame_descriptor.shape)\n",
        "\n",
        "        distances = torch.norm(cluster_descriptors - frame_descriptor, dim=1)\n",
        "\n",
        "        # Find the index of the minimum distance (i.e., closest cluster descriptor)\n",
        "        predicted_label = torch.argmin(distances)\n",
        "        predicted_labels.append(predicted_label.item())\n",
        "\n",
        "    # Convert predicted labels list to a tensor on the same device\n",
        "    return predicted_label.detach().cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "qe5IWDaLVesG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "mead_mfcc_path = \"/content/drive/MyDrive/MEAD_MFCC/\"\n",
        "# Load and prepare data\n",
        "dataset_path = \"DatasetForLSTM/\"+ Zipped_inside_folder\n",
        "\n",
        "# Update video paths\n",
        "# Go through each video name and update path of the csv --> wave to vec csvs\n",
        "all_video_paths = []\n",
        "for video in videos_list:\n",
        "  if 'M' in video: #For MEAD\n",
        "    all_video_paths.append(mead_mfcc_path+video+\".csv\")\n",
        "  else:\n",
        "    all_video_paths.append(\"DatasetForLSTM/\"+Zipped_inside_folder+\"/\"+video+\".csv\")\n",
        "\n",
        "train_video_paths, test_video_paths = train_test_split(all_video_paths, test_size=0.05, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QizkOQbIWeRU",
        "outputId": "e4a174c4-ca2a-4cae-97fe-f1907f23ac76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i_i in [0,1,2]:\n",
        "#   print(i_i)\n",
        "matched_train = 0\n",
        "total_train = 0\n",
        "for video_path in train_video_paths:\n",
        "  df = pd.read_csv(video_path, header=None)\n",
        "  features = df.iloc[0, :].values.astype(np.float32)\n",
        "  closest_neighbors, closest_neighbors_mfccs = KNNPrediction(features.reshape(1,-1), knn, clusters_labels_, cluster_mfcc_rep)\n",
        "  original_label = frames_data[video_name][0]\n",
        "  if K ==1:\n",
        "    if original_label == closest_neighbors[0]:\n",
        "      matched_train +=1\n",
        "    total_train +=1\n",
        "    continue\n",
        "\n",
        "  descriptor_list = []\n",
        "  for descriptor in closest_neighbors_mfccs:\n",
        "    descriptor_list.append(descriptor)\n",
        "  descriptor_list = np.vstack(descriptor_list)\n",
        "\n",
        "  descriptor_list = np.mean(descriptor_list, axis=0, keepdims=True)\n",
        "  descriptor_list = torch.tensor(descriptor_list, device=device,  dtype=torch.float32)\n",
        "  predicted_labels = get_predicted_labels(descriptor_list, vectors, device)\n",
        "  if original_label == predicted_labels:\n",
        "    matched_train +=1\n",
        "  total_train +=1\n",
        "\n",
        "print(\"Accuracy on train:\", (matched_train*100)/total_train,\"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "for video_path in test_video_paths:\n",
        "  df = pd.read_csv(video_path, header=None)\n",
        "  features = df.iloc[0, :].values.astype(np.float32)\n",
        "  closest_neighbors, closest_neighbors_mfccs = KNNPrediction(features.reshape(1,-1), knn, clusters_labels_, cluster_mfcc_rep)\n",
        "  original_label = frames_data[video_name][0]\n",
        "  if K ==1:\n",
        "    if original_label == closest_neighbors[0]:\n",
        "      matched_test +=1\n",
        "    total_test +=1\n",
        "    continue\n",
        "\n",
        "  descriptor_list = []\n",
        "  for descriptor in closest_neighbors_mfccs:\n",
        "    descriptor_list.append(descriptor)\n",
        "  descriptor_list = np.vstack(descriptor_list)\n",
        "\n",
        "  descriptor_list = np.mean(descriptor_list, axis=0, keepdims=True)\n",
        "  descriptor_list = torch.tensor(descriptor_list, device=device,  dtype=torch.float32)\n",
        "  predicted_labels = get_predicted_labels(descriptor_list, vectors, device)\n",
        "  if original_label == predicted_labels:\n",
        "    matched_test +=1\n",
        "  total_test +=1\n",
        "\n",
        "print(\"Accuracy on test:\", (matched_test*100)/total_test,\"%\")\n",
        "\n",
        "\n",
        "print(\"Accuracy on Test + Train:\", ((matched_test + matched_train)*100)/(total_test+total_train),\"%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bDmwdsN9frk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "K=3\n",
        "Level 1:\n",
        "Level 2:\n",
        "Level 3:\n",
        "Level 4:\n",
        "\n",
        "K=1\n",
        "Level 1: 32%\n",
        "Level 2: 20.65%\n",
        "Level 3: 17%\n",
        "Level 4: 15.6%\n",
        "'''"
      ],
      "metadata": {
        "id": "-Lsx7oLlSavB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebKWiAwbWhDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}