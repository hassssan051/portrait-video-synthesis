{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassssan051/portrait-video-synthesis/blob/audio-to-descriptor-pred/prediction/NN_First_Frame_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_9Lm2sJklOx",
        "outputId": "28762dbc-38af-4731-d0c0-035eea0ba932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEG3twbEksTV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGVT1IvFktfK"
      },
      "outputs": [],
      "source": [
        "#loading MFCC features of RAVDESS dataset\n",
        "datasetPath = 'DatasetForLSTM'\n",
        "Zipped_inside_folder = 'RAVDESS_MFCC'\n",
        "with zipfile.ZipFile('drive/MyDrive/'+Zipped_inside_folder+'.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(datasetPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcjOYNFTkvH6",
        "outputId": "f55eb581-cfd5-484a-94cc-61c201c17fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9282\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "\n",
        "#getting descriptors of frames of each video and storing this information in a dictionary (true_descriptors) where key is video name and value is a list of descriptors of its frames.\n",
        "\n",
        "clusters_info = 'Sawaiz_2/pkl_for_lstm_encoded/17_53_50_800'\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/live_portrait_descriptors_all_encoder.pkl'\n",
        "\n",
        "# Open the file in binary read mode and load the data\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "video_dict = defaultdict(list)\n",
        "\n",
        "\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in data.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    video_dict[video_name].append((frame_number, value))\n",
        "\n",
        "\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "final_video_dict = {}\n",
        "for video_name, frames in video_dict.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [frame_data for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    final_video_dict[video_name] = np.vstack(sorted_arrays)\n",
        "true_descriptors = final_video_dict\n",
        "videos_list = list(true_descriptors.keys())\n",
        "print(len(videos_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLHH9imjkwf7"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/averaged_descriptors_encoded.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/frame_to_cluster_mapping.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbM7P0XFk3KL"
      },
      "outputs": [],
      "source": [
        "for key, value in frames_data.items():\n",
        "    sorted_value = sorted(value, key=lambda x: int(x[0].split('_')[-1].split('.')[0]) if '_' in x[0] else int(x[0].split('.')[0]))\n",
        "    frames_data[key] = sorted_value\n",
        "\n",
        "for key, val in frames_data.items():\n",
        "    frames_data[key] = [ x[1] for x in val]\n",
        "\n",
        "frames_data_new = {}\n",
        "for key, val in frames_data.items():\n",
        "    if 'M' in key:\n",
        "      parts = key.split(\"/\")\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frames_data_new[video_name] = val\n",
        "    else:\n",
        "      frames_data_new[key] = val\n",
        "frames_data = frames_data_new\n",
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wldeDqX3FllK",
        "outputId": "b28a153f-fa30-4e31-e397-a5244dcad131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9282\n"
          ]
        }
      ],
      "source": [
        "print(len(list(frames_data.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpGUqbwVzAbc",
        "outputId": "a3164b62-5188-4aed-f7a0-3c853f3ada81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "760\n",
            "727\n",
            "395\n",
            "12.213157894736842\n"
          ]
        }
      ],
      "source": [
        "frames_count = {}\n",
        "first_frame_clusters = {}\n",
        "only_mead = {}\n",
        "only_ravdess = {}\n",
        "for key, val in frames_data.items():\n",
        "  first_frame_clusters[val[0]] = 1\n",
        "  if 'M' in key:\n",
        "    only_mead[val[0]] =1\n",
        "  else:\n",
        "    only_ravdess[val[0]] = 1\n",
        "  if val[0] not in frames_count:\n",
        "    frames_count[val[0]] = 0\n",
        "  frames_count[val[0]] += 1\n",
        "\n",
        "print(len(list(first_frame_clusters.keys())))\n",
        "print(len(list(only_mead.keys())))\n",
        "print(len(list(only_ravdess.keys())))\n",
        "vals = (list(frames_count.values()))\n",
        "vals.sort(reverse= True)\n",
        "valsnp = np.array(vals)\n",
        "print(np.mean(valsnp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL0QERRkk77z"
      },
      "outputs": [],
      "source": [
        "number_of_clusters = 800 # Updated\n",
        "stacked_descriptors = [clusters_data[val] for val in range(number_of_clusters)]\n",
        "clusters_descriptors = np.vstack(stacked_descriptors) #A numpy array of cluster decriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d725I1SQk3mz"
      },
      "outputs": [],
      "source": [
        "#Here, key is a video name and value is a list of cluster representatives of those clusters to which its frames are mapped\n",
        "\n",
        "clusters_rep_as_ground_truth_for_a_video = {}\n",
        "for video, frames in frames_data.items():\n",
        "  stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "  clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/cluster_rep_clusters4_level1_clusters24_level2.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/frame_to_cluster_clusters4_level1_clusters24_level2.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data_raw = pickle.load(file)\n",
        "\n",
        "cluster_level = 4\n",
        "frames_to_clusters_indices = {}\n",
        "clusters_indices= {}\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in frames_data_raw.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    if video_name not in frames_to_clusters_indices:\n",
        "      frames_to_clusters_indices[video_name] = []\n",
        "    try:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "    except:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "\n",
        "\n",
        "\n",
        "clusters_descriptors = []\n",
        "idx = 0\n",
        "for key, val in clusters_indices.items():\n",
        "  clusters_indices[key] = key\n",
        "  #print(key)\n",
        "  clusters_descriptors.append(clusters_data[key])\n",
        "  idx+=1\n",
        "clusters_descriptors = np.vstack(clusters_descriptors)\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "frames_data = {}\n",
        "for video_name, frames in frames_to_clusters_indices.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [clusters_indices[frame_data] for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    frames_data[video_name] = sorted_arrays\n",
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames.\n",
        "\n",
        "# clusters_rep_as_ground_truth_for_a_video = {}\n",
        "# for video, frames in frames_data.items():\n",
        "#   stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "#   clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ],
      "metadata": {
        "id": "7G5Dzfm9ucaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hierarchical_dict(clusters_data, level_number):\n",
        "    \"\"\"\n",
        "    Create a hierarchical dictionary of clusters and their children up to the specified level.\n",
        "\n",
        "    Args:\n",
        "        clusters_data (dict): Dictionary where keys are cluster names and values are their representatives.\n",
        "        level_number (int): Specifies the depth of hierarchy to include in the output.\n",
        "\n",
        "    Returns:\n",
        "        dict: Hierarchical dictionary with clusters grouped by levels.\n",
        "    \"\"\"\n",
        "    hierarchical_dict = {}\n",
        "\n",
        "    # Step 1: Extract Level 1 clusters\n",
        "    level_1_clusters = {name: rep for name, rep in clusters_data.items() if name.startswith(\"Cluster_\") and \".\" not in name}\n",
        "    hierarchical_dict[\"Level 1\"] = level_1_clusters\n",
        "    for i in range(2, level_number):\n",
        "      level_clusters = {name: rep for name, rep in clusters_data.items() if name.startswith(\"Cluster_\") and name.count('.')==i}\n",
        "      hierarchical_dict[f\"Level {i}\"] = level_clusters\n",
        "    # Step 2: Extract children for each cluster iteratively up to the specified level\n",
        "    def extract_children(parent_cluster):\n",
        "        \"\"\"Extract direct child clusters of a given parent cluster.\"\"\"\n",
        "        parent_prefix = parent_cluster + \".\"\n",
        "        return {name: rep for name, rep in clusters_data.items() if name.startswith(parent_prefix) and name.count('.') == parent_cluster.count('.') + 1}\n",
        "\n",
        "    # Build hierarchy up to the specified level\n",
        "    for current_level in range(1, level_number):\n",
        "        current_clusters = hierarchical_dict.get(\"Level 1\" if current_level == 1 else f\"Level {current_level}\", {})\n",
        "        for parent_cluster in current_clusters:\n",
        "            children = extract_children(parent_cluster)\n",
        "            if children:\n",
        "                hierarchical_dict[parent_cluster] = children\n",
        "\n",
        "    return hierarchical_dict\n",
        "\n",
        "hierarchical_cluster_data =create_hierarchical_dict(clusters_data, 2)"
      ],
      "metadata": {
        "id": "Bn8LzwN2qjn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRLuaUqok5k8"
      },
      "outputs": [],
      "source": [
        "def extract_wav2vec_features_and_labels_from_csv(csv_path, scaler=None): #To get audio features (MFCC/Wav2Vec) of a video\n",
        "    df = pd.read_csv(csv_path, header=None)\n",
        "    features = df.iloc[:-1, :].values.astype(np.float32)\n",
        "    video_name = csv_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "    if scaler is not None:\n",
        "        features = scaler.transform(features)\n",
        "\n",
        "    return features, true_descriptors[video_name]#frames_data[video_name]#clusters_rep_as_ground_truth_for_a_video[video_name]#true_descriptors[video_name]\n",
        "\n",
        "def load_data_from_directory(directory_path):\n",
        "    return [os.path.join(directory_path, fname) for fname in os.listdir(directory_path) if fname.endswith(\".csv\")]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item[0] for item in batch]\n",
        "    # for regression\n",
        "    labels = [item[1] for item in batch]\n",
        "    features_padded = torch.nn.utils.rnn.pad_sequence(features, batch_first=True)\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return features_padded, labels_padded#torch.tensor(labels, dtype=torch.int64)#labels_padded\n",
        "\n",
        "first_frames = 3\n",
        "class SlidingWindowAudioDataset(Dataset):\n",
        "    def __init__(self, video_paths, num_clusters=500, scaler=None):\n",
        "        \"\"\"\n",
        "        Dataset for audio features with sliding window implementation.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.data = []  # To store concatenated feature-label vectors\n",
        "        self.labels = []  # To store the corresponding output labels (Nth label)\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for video_path in self.video_paths:\n",
        "            features, labels = extract_wav2vec_features_and_labels_from_csv(video_path, self.scaler)\n",
        "\n",
        "            # Normalize features using the fitted scaler\n",
        "            if self.scaler:\n",
        "                features = self.scaler.transform(features)\n",
        "\n",
        "            # For regression\n",
        "            num_rows = labels.shape[0]\n",
        "            # For classification\n",
        "            #num_rows = len(labels)\n",
        "            #For All data\n",
        "            # for i in range(num_rows):\n",
        "            #     try:\n",
        "            #       window_features = features[i]\n",
        "            #       target_label = labels[i]\n",
        "\n",
        "            #       input_vector = window_features.flatten()\n",
        "\n",
        "            #       # The Nth label is the target/output\n",
        "\n",
        "\n",
        "            #       self.data.append(input_vector)\n",
        "            #       self.labels.append(target_label)\n",
        "            #     except:\n",
        "            #       continue\n",
        "\n",
        "            #For first frame only\n",
        "            self.data.append(features[:first_frames].flatten())\n",
        "            self.labels.append(labels[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.data[idx], dtype=torch.float32),  # The concatenated feature-label vector\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float32),  # For regression\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ZHbcGwlaPe",
        "outputId": "10047686-1c31-4798-fe6f-6b0c4acb1e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Scaler loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "import joblib\n",
        "\n",
        "# Save the trained scaler to a file\n",
        "scaler_filename = \"/content/drive/MyDrive/Sawaiz_2/results_pkl_videos/scaler_MEAD2.pkl\"\n",
        "# joblib.dump(scaler, scaler_filename)\n",
        "# print(f\"Scaler saved to {scaler_filename}\")\n",
        "scaler = joblib.load(scaler_filename)\n",
        "print(\"Scaler loaded successfully!\")\n",
        "mead_mfcc_path = \"/content/drive/MyDrive/MEAD_MFCC/\"\n",
        "# Load and prepare data\n",
        "dataset_path = \"DatasetForLSTM/\"+ Zipped_inside_folder\n",
        "\n",
        "# Update video paths\n",
        "# Go through each video name and update path of the csv --> wave to vec csvs\n",
        "all_video_paths = []\n",
        "for video in videos_list:\n",
        "  if 'M' in video: #For MEAD\n",
        "    all_video_paths.append(mead_mfcc_path+video+\".csv\")\n",
        "  else:\n",
        "    all_video_paths.append(\"DatasetForLSTM/\"+Zipped_inside_folder+\"/\"+video+\".csv\")\n",
        "\n",
        "train_video_paths, test_video_paths = train_test_split(all_video_paths, test_size=0.05, random_state=42)\n",
        "# for video_path in train_video_paths:\n",
        "#     features, _ = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "    #scaler.partial_fit(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQrCz2UCldCG"
      },
      "outputs": [],
      "source": [
        "#Batch Size\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Initialize the sliding window dataset\n",
        "train_dataset = SlidingWindowAudioDataset(train_video_paths, scaler=scaler)\n",
        "test_dataset = SlidingWindowAudioDataset(test_video_paths, scaler=scaler)\n",
        "\n",
        "# DataLoader remains the same\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYREwOZAUq_7",
        "outputId": "0cd9af55-f089-494a-db4e-70192b58cebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17037\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJDa7eQWctnI"
      },
      "source": [
        "# NN for Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRtnuuTUlwkA",
        "outputId": "d31a9e54-a375-4c3a-8569-0f258e33d2d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputNN(\n",
              "  (fc1): Linear(in_features=84, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc6): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc7): Linear(in_features=32, out_features=16, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "class MultiOutputNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Multi-output neural network that outputs an N-dimensional vector.\n",
        "\n",
        "        Args:\n",
        "        - input_dim: Number of input features.\n",
        "        - output_dim: Size of the output vector (N).\n",
        "        \"\"\"\n",
        "        super(MultiOutputNN, self).__init__()\n",
        "\n",
        "        # Define the layers of the network\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64,128)\n",
        "        self.fc3 = nn.Linear(128, 256)\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 32)         # Second hidden layer\n",
        "        self.fc7 = nn.Linear(32, output_dim)  # Output layer (N-dimensional vector)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "        - output: Tensor of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = torch.relu(self.fc6(x))\n",
        "        output = self.fc7(x)         # Output layer (no activation)\n",
        "        return output\n",
        "\n",
        "model = MultiOutputNN(28*first_frames,16).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Sawaiz_2/saved_models/NN_firstframe__first3frames_GT:frames_800_hierarchical_clusters.pth',map_location=torch.device('cpu'), weights_only=True)) #Reload path add\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljHmdcfMl06Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device, alpha=0.5, beta=0.5):\n",
        "    model.train()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(train_loader)\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (features, labels) in enumerate(progress_bar):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        if outputs.shape[1] != labels.shape[1]:\n",
        "          outputs = outputs[:, :-1, :]\n",
        "\n",
        "        # Compute MSE loss\n",
        "        mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mse_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "        # Update progress bar with average batch loss so far\n",
        "        progress_bar.set_postfix({\n",
        "\n",
        "            \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "        })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "\n",
        "    print(f\"Training Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "    return avg_mse_loss\n",
        "\n",
        "def test(model, test_loader, device, alpha=0.5, beta=0.5):\n",
        "    model.eval()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(test_loader)\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, labels) in enumerate(progress_bar):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            if outputs.shape[1] != labels.shape[1]:\n",
        "              outputs = outputs[:, :-1, :]\n",
        "\n",
        "            # Compute MSE loss\n",
        "            mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "\n",
        "            cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "\n",
        "            # Update progress bar with average batch loss so far\n",
        "            progress_bar.set_postfix({\n",
        "\n",
        "                \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "\n",
        "            })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "    print(f\"Testing Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "\n",
        "    return avg_mse_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae4wqYMcSUt"
      },
      "source": [
        "# For Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jj7s6ZJcSC5"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, device, alpha=0.5, beta=0.5):\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss() #For classification\n",
        "    cumulative_loss = 0  # Cumulative loss for the epoch\n",
        "    num_batches = len(train_loader)\n",
        "    correct_predictions = 0  # For accuracy calculation\n",
        "    total_predictions = 0  # For accuracy calculation\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (features, labels) in enumerate(progress_bar):\n",
        "        labels = labels.long()\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        # Accumulate the loss\n",
        "        cumulative_loss += loss.item()\n",
        "\n",
        "        # Update progress bar with average batch loss and accuracy so far\n",
        "        progress_bar.set_postfix({\n",
        "            \"Average Loss\": cumulative_loss / (i + 1),\n",
        "            \"Accuracy\": 100. * correct_predictions / total_predictions\n",
        "        })\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    avg_loss = cumulative_loss / num_batches\n",
        "    accuracy = 100. * correct_predictions / total_predictions\n",
        "\n",
        "    print(f\"Training Epoch Average Loss: {avg_loss}, Accuracy: {accuracy}%\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(model, test_loader, device, alpha=0.5, beta=0.5):\n",
        "    model.eval()\n",
        "    criterion = torch.nn.CrossEntropyLoss() #For classification\n",
        "    cumulative_loss = 0  # Cumulative loss for the epoch\n",
        "    correct_predictions = 0  # For accuracy calculation\n",
        "    total_predictions = 0  # For accuracy calculation\n",
        "    num_batches = len(test_loader)\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, labels) in enumerate(progress_bar):\n",
        "            labels = labels.long()\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "            # Accumulate the loss\n",
        "            cumulative_loss += loss.item()\n",
        "\n",
        "            # Update progress bar with average batch loss and accuracy so far\n",
        "            progress_bar.set_postfix({\n",
        "                \"Average Loss\": cumulative_loss / (i + 1),\n",
        "                \"Accuracy\": 100. * correct_predictions / total_predictions\n",
        "            })\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    avg_loss = cumulative_loss / num_batches\n",
        "    accuracy = 100. * correct_predictions / total_predictions\n",
        "\n",
        "    print(f\"Testing Epoch Average Loss: {avg_loss}, Accuracy: {accuracy}%\")\n",
        "    return avg_loss, accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im0P58TC-Rz8"
      },
      "source": [
        "# COSINE ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj7J4eXq-T9I"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device, alpha=0.5, beta=0.5):\n",
        "    model.train()\n",
        "    cumulative_cosine_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(train_loader)\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (features, labels) in enumerate(progress_bar):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        if outputs.shape[1] != labels.shape[1]:\n",
        "          outputs = outputs[:, :-1, :]\n",
        "\n",
        "        # Compute Cosine Similarity loss\n",
        "        cosine_loss = 1 - F.cosine_similarity(outputs, labels, dim=-1).mean()\n",
        "\n",
        "\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cosine_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        cumulative_cosine_loss += cosine_loss.item()\n",
        "\n",
        "        # Update progress bar with average batch loss so far\n",
        "        progress_bar.set_postfix({\n",
        "\n",
        "            \"Average COSINE\": cumulative_cosine_loss / (i + 1)\n",
        "        })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_cosine_loss = cumulative_cosine_loss / num_batches\n",
        "\n",
        "    print(f\"Training Epoch Average COSINE Loss: {avg_cosine_loss}\", sep=\" \")\n",
        "    return avg_cosine_loss\n",
        "\n",
        "def test(model, test_loader, device, alpha=0.5, beta=0.5):\n",
        "    model.eval()\n",
        "    cumulative_cosine_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(test_loader)\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, labels) in enumerate(progress_bar):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            if outputs.shape[1] != labels.shape[1]:\n",
        "              outputs = outputs[:, :-1, :]\n",
        "\n",
        "            # Compute Cosine Similarity loss\n",
        "            cosine_loss = 1 - F.cosine_similarity(outputs, labels, dim=-1).mean()\n",
        "\n",
        "\n",
        "            cumulative_cosine_loss += cosine_loss.item()\n",
        "\n",
        "\n",
        "            # Update progress bar with average batch loss so far\n",
        "            progress_bar.set_postfix({\n",
        "\n",
        "                \"Average COSINE\": cumulative_cosine_loss / (i + 1)\n",
        "\n",
        "            })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_cosine_loss = cumulative_cosine_loss / num_batches\n",
        "    print(f\"Testing Epoch Average COSINE Loss: {avg_cosine_loss}\", sep=\" \")\n",
        "\n",
        "    return avg_cosine_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp38hTi5l2ng",
        "outputId": "e8c2a694-3fe9-4d85-9a6a-05b608fb1ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Optimizer, criterion, and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QTXQ-odl4Lw"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "# model.load_state_dict(torch.load('drive/MyDrive/LSTM_Params/lstm_predicting_LP_descriptors.pth', weights_only=True))\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, device, alpha=1, beta=0.001)\n",
        "    test_loss = test(model, test_loader, device, alpha=1, beta=0.001)\n",
        "\n",
        "    #Step the scheduler with the test loss\n",
        "    # scheduler.step(test_loss)\n",
        "\n",
        "#     if test_loss < best_loss:\n",
        "#         best_loss = test_loss\n",
        "#         torch.save(model.state_dict(), 'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_best_model.pth')\n",
        "#         print(f\"New best model saved with loss: {best_loss:.2f}\")\n",
        "\n",
        "#     if epoch % 5 == 0:\n",
        "#         torch.save(model.state_dict(), f'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_epoch{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr7Qn4tF0UhX"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/Sawaiz_2/saved_models/NN_firstframe__usingfirst3frames_800_clusters.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL7ysR3ql6ho"
      },
      "outputs": [],
      "source": [
        "class PerVideoPerFrameDatasetForAccuracy(Dataset):\n",
        "    def __init__(self, video_paths, frames_to_clusters_mapping, scaler=None):\n",
        "        \"\"\"\n",
        "        Dataset for generating sliding windows for an entire video.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.frames_to_clusters_mapping = frames_to_clusters_mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        features, labels = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "        video_name = video_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "        # Normalize features using the scaler\n",
        "        if self.scaler:\n",
        "            features = self.scaler.transform(features)\n",
        "\n",
        "        # for regression\n",
        "        #num_rows = labels.shape[0]\n",
        "        #for classification\n",
        "        #num_rows = len(labels)\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        # For all frames\n",
        "        # for i in range(num_rows):\n",
        "        #     window_features = features[i]\n",
        "        #     target_label = frames_data[video_name][i]\n",
        "\n",
        "        #     # Concatenate the first (N-1) labels with the N features for input\n",
        "        #     # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "        #     input_vector = window_features.flatten()\n",
        "        #     all_features.append(input_vector)\n",
        "        #     # The Nth label is the target/output\n",
        "\n",
        "        #     all_labels.append(target_label)\n",
        "            #For first frame only\n",
        "            #break\n",
        "\n",
        "        #For first frame only\n",
        "        all_features.append(features[:first_frames].flatten())\n",
        "        all_labels.append(frames_data[video_name][0])\n",
        "\n",
        "        return {\n",
        "            \"video\": video_name,\n",
        "            \"features\": [torch.tensor(w, dtype=torch.float32) for w in all_features],\n",
        "            \"labels\": [w for w in all_labels],\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsnjhq13m-to"
      },
      "outputs": [],
      "source": [
        "def per_video_per_frame_collate_fn(batch):\n",
        "    video_name = batch[0]['video']\n",
        "    features = batch[0][\"features\"]  # List of sliding windows for each video\n",
        "    labels = batch[0][\"labels\"]  # Corresponding labels\n",
        "    return video_name, features, labels\n",
        "\n",
        "\n",
        "# Initialize the sliding window per video dataset\n",
        "train_dataset_ = PerVideoPerFrameDatasetForAccuracy(train_video_paths, frames_data, scaler=scaler)\n",
        "test_dataset_ = PerVideoPerFrameDatasetForAccuracy(test_video_paths,frames_data, scaler=scaler)\n",
        "\n",
        "# Initialize the data loaders\n",
        "train_loader_ = DataLoader(train_dataset_, batch_size=1, shuffle=False, collate_fn=per_video_per_frame_collate_fn)\n",
        "test_loader_ = DataLoader(test_dataset_, batch_size=1, shuffle=False, collate_fn=per_video_per_frame_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N3RN9QWnR8H"
      },
      "outputs": [],
      "source": [
        "def get_predicted_labels(outputs, cluster_descriptors, device):\n",
        "    # Ensure cluster_descriptors is a torch tensor and move to device\n",
        "    cluster_descriptors = torch.tensor(cluster_descriptors, device=device, dtype=outputs.dtype)\n",
        "    #print(cluster_descriptors.shape)\n",
        "    # Initialize an empty list to store predicted labels for each frame\n",
        "    predicted_labels = []\n",
        "    #outputs = outputs.squeeze(0)\n",
        "    # Iterate over each frame descriptor in outputs\n",
        "    for frame_descriptor in outputs:\n",
        "        # Calculate Euclidean distances between the frame descriptor and each cluster descriptor\n",
        "\n",
        "        distances = torch.norm(cluster_descriptors - frame_descriptor, dim=1)\n",
        "\n",
        "        # Find the index of the minimum distance (i.e., closest cluster descriptor)\n",
        "        predicted_label = torch.argmin(distances)\n",
        "        predicted_labels.append(predicted_label.item())\n",
        "\n",
        "    # Convert predicted labels list to a tensor on the same device\n",
        "    return torch.tensor(predicted_labels, device=device)\n",
        "\n",
        "def evaluate_video_predictions(outputs, cluster_descriptors, actual_labels_names, device, cluster_nearest_neighbors):\n",
        "    # Get predicted labels based on Euclidean distance comparison\n",
        "    predicted_labels = get_predicted_labels(outputs, cluster_descriptors, device)\n",
        "    # Get the actual labels for the frames from frames_data and convert to torch tensor on the same device\n",
        "    #print(predicted_labels.shape)\n",
        "    actual_labels = torch.tensor(actual_labels_names, device=device)\n",
        "    # Initialize a counter for matches\n",
        "    matches = 0\n",
        "    total_labels = actual_labels.size(0)\n",
        "    # Iterate through each label and check for matches or nearest neighbor matches\n",
        "    for i in range(total_labels):\n",
        "        actual_label = actual_labels[i].item()\n",
        "        predicted_label = predicted_labels[i].item()\n",
        "\n",
        "        # Check if predicted label is a match or a match with the nearest neighbor\n",
        "        if predicted_label == actual_label or  predicted_label in cluster_nearest_neighbors[actual_label]:\n",
        "            matches += 1\n",
        "    predicted_labels = predicted_labels.cpu().numpy()\n",
        "    return predicted_labels.tolist(), matches, total_labels\n",
        "\n",
        "def evaluation_for_classification(outputs, actual_labels_names, device):\n",
        "  _, predicted_labels = torch.max(outputs, 1)\n",
        "  actual_labels = torch.tensor(actual_labels_names, device=device)\n",
        "  # Initialize a counter for matches\n",
        "  matches = 0\n",
        "  total_labels = actual_labels.size(0)\n",
        "  # Iterate through each label and check for matches or nearest neighbor matches\n",
        "  for i in range(total_labels):\n",
        "      actual_label = actual_labels[i].item()\n",
        "      predicted_label = predicted_labels[i].item()\n",
        "\n",
        "      # Check if predicted label is a match or a match with the nearest neighbor\n",
        "      if predicted_label == actual_label or  predicted_label in cluster_nearest_neighbors[actual_label]:\n",
        "          matches += 1\n",
        "  predicted_labels = predicted_labels.cpu().numpy()\n",
        "  return predicted_labels.tolist(), matches, total_labels\n",
        "\n",
        "def find_closest_cluster(hierarchical_dict, vector, level_number):\n",
        "    \"\"\"\n",
        "    Find the closest cluster to a given vector up to a specified level in the hierarchy.\n",
        "\n",
        "    Args:\n",
        "        hierarchical_dict (dict): Hierarchical dictionary of clusters and their children.\n",
        "        vector (np.ndarray): The input vector (1, n) to compare against cluster representatives.\n",
        "        level_number (int): The level number up to which to find the closest cluster.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the closest cluster.\n",
        "    \"\"\"\n",
        "    current_level_clusters = hierarchical_dict.get(\"Level 1\", {})\n",
        "    closest_cluster = None\n",
        "\n",
        "\n",
        "    for level in range(1, level_number + 1):\n",
        "        closest_distance = float(\"inf\")\n",
        "        if not current_level_clusters:\n",
        "            break\n",
        "\n",
        "        for cluster_name, cluster_rep in current_level_clusters.items():\n",
        "            cluster_rep = np.array(cluster_rep)  # Ensure the cluster representative is a numpy array\n",
        "            distance = np.linalg.norm(vector - cluster_rep)  # Euclidean distance\n",
        "            if distance < closest_distance:\n",
        "                closest_distance = distance\n",
        "                closest_cluster = cluster_name\n",
        "\n",
        "        # Stop if the current level is the maximum level specified\n",
        "        if level == level_number:\n",
        "            break\n",
        "\n",
        "        # Move to the next level: get children of the closest cluster\n",
        "        current_level_clusters = hierarchical_dict.get(closest_cluster, {})\n",
        "\n",
        "    return closest_cluster\n",
        "def evaluation_with_hierarchical_clustering(hierarchical_dict, vector, level_number, actual_label):\n",
        "  predicted_label = find_closest_cluster(hierarchical_dict, vector, level_number)\n",
        "  matched = 0\n",
        "  if predicted_label == actual_label:\n",
        "    matched+=1\n",
        "  return [predicted_label], matched, 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmtAJmyKnwCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Compute pairwise Euclidean distances\n",
        "pairwise_distances = squareform(pdist(clusters_descriptors, metric='euclidean'))\n",
        "\n",
        "# Set diagonal to infinity to ignore self-distances\n",
        "np.fill_diagonal(pairwise_distances, np.inf)\n",
        "\n",
        "# Find the indices of the top 5 nearest neighbors for each cluster\n",
        "cluster_nearest_neighbors = np.argsort(pairwise_distances, axis=1)[:, :1].tolist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2xumN2L5zYm"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Sawaiz_2/saved_models/NN_firstframe__usingfirst3frames_800_clusters.pth', weights_only=True)) #Reload path add\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JscFOiq7nUcp",
        "outputId": "8686b85a-1c3f-4513-a5ef-48d777cb9404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy :  5.126460247249631 %\n",
            "Test Accuracy :  3.6559139784946235 %\n",
            "Test Accuracy (Test + Training):  5.052790346907994 %\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "matched = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "for video_name, features_list, actual_labels in train_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      outputs = model(features)  # Obtain model predictions directly in torch\n",
        "      #for hierarchical clustering\n",
        "      outputs = outputs.detach().numpy()\n",
        "      predicted_labels, matches, total_labels =evaluation_with_hierarchical_clustering(hierarchical_cluster_data, outputs, 2, actual_labels[0])\n",
        "      # for regression\n",
        "      #predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i]], device, cluster_nearest_neighbors)\n",
        "      #for classification\n",
        "      #predicted_labels, matches, total_labels = evaluation_for_classification(outputs, [actual_labels[i]], device)\n",
        "      labels_for_video.extend(predicted_labels)\n",
        "      matched+=matches\n",
        "      total += total_labels\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Train Accuracy : \", (matched * 100)/total, \"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "\n",
        "for video_name, features_list, actual_labels in test_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      outputs = model(features)  # Obtain model predictions directly in torch\n",
        "      #for hierarchical clustering\n",
        "      outputs = outputs.detach().numpy()\n",
        "      predicted_labels, matches, total_labels =evaluation_with_hierarchical_clustering(hierarchical_cluster_data, outputs, 2, actual_labels[0])\n",
        "      # for regression\n",
        "      #predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i]], device, cluster_nearest_neighbors)\n",
        "      #for classification\n",
        "      #predicted_labels, matches, total_labels = evaluation_for_classification(outputs, [actual_labels[i]], device)\n",
        "      labels_for_video.extend(predicted_labels)\n",
        "      matched_test+=matches\n",
        "      total_test += total_labels\n",
        "\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Test Accuracy : \", (matched_test*100)/total_test, \"%\")\n",
        "\n",
        "print(\"Test Accuracy (Test + Training): \", ((matched_test+matched)*100)/(total_test+total), \"%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV7JQDwLqU9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc87605b-9e8a-47a3-d564-3ebfb7acfc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8817\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "4 Clusters Level in 1 and 16 Clusters Level 2: 8.98%\n",
        "4 Clusters Level in 1 and 20 Cluster Level 2: 8.75%\n",
        "4 Clusters Level in 1 and 24 Cluster Level 2: 5%\n",
        "\n",
        "5 Clusters Level in 1 and 16 Clusters Level 2: 8,7%\n",
        "5 Clusters Level in 1 and 20 Clusters Level 2: 4.98%\n",
        "5 Clusters Level in 1 and 24 Clusters Level 2: 6.82%\n",
        "'''\n",
        "\n",
        "'''\n",
        "4 Clusters: 90%\n",
        "5 Clusters: 83.3%\n",
        "6 Clusters: 42%\n",
        "7 Clusters: 65.6%\n",
        "8 Clusters: 64.1%\n",
        "9 Clusters: 66.9%\n",
        "10 Clusters: 39.7%\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3e0nomU3LBhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXXSkLZHpz8s9AD3l/PTp9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}