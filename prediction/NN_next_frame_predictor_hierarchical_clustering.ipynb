{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassssan051/portrait-video-synthesis/blob/audio-to-descriptor-pred/prediction/NN_next_frame_predictor_hierarchical_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzv9Btt_6VRb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl8xxzax6Y08"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st49ZqezbfuX"
      },
      "outputs": [],
      "source": [
        "#loading MFCC features of RAVDESS dataset\n",
        "datasetPath = 'DatasetForLSTM'\n",
        "Zipped_inside_folder = 'RAVDESS_MFCC'\n",
        "with zipfile.ZipFile('drive/MyDrive/'+Zipped_inside_folder+'.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(datasetPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rve1Gkdzbf9X",
        "outputId": "23c7ba0e-aacc-413f-e20c-f547b8a64454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9282\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "#getting descriptors of frames of each video and storing this information in a dictionary where key is video name and value is a list of descriptors of its frames.\n",
        "\n",
        "\n",
        "clusters_info = 'Sawaiz_2/pkl_for_lstm_encoded/17_53_50_800'\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/live_portrait_descriptors_all_encoder.pkl'\n",
        "\n",
        "# Open the file in binary read mode and load the data\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "video_dict = defaultdict(list)\n",
        "\n",
        "\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in data.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    video_dict[video_name].append((frame_number, value))\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "final_video_dict = {}\n",
        "for video_name, frames in video_dict.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [frame_data for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    final_video_dict[video_name] = np.vstack(sorted_arrays)\n",
        "true_descriptors = final_video_dict\n",
        "videos_list = list(true_descriptors.keys())\n",
        "print(len(videos_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All descriptors of all frames"
      ],
      "metadata": {
        "id": "jtysVeAwGTiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uagkEGSMbhn1"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/sub_cluster_average_vectors.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/hierarchical_clusters_frame_to_cluster.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data_raw = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6v5Lcp-bnws"
      },
      "source": [
        "# Setting Indices as clusterIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByurYgRVGeUv",
        "outputId": "61423a22-5180-4321-ac97-f3e98b502eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "print(clusters_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output of heirarchical\n",
        "# Frame to cluster\n",
        "\n",
        "# I need cluster of each frame in each video in the format: Key video name and the value is the list of cluster id\n",
        "# Tree clusters\n",
        "#"
      ],
      "metadata": {
        "id": "JuCaSh9IGgXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2RnFT_Xsck53",
        "outputId": "990aaf61-d74b-4b1c-dbf0-51afac6df865"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'cluster_2.cluster_2.cluster_1.cluster_1'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-e6befd0457a3>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mclusters_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mclusters_descriptors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0midx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'cluster_2.cluster_2.cluster_1.cluster_1'"
          ]
        }
      ],
      "source": [
        "cluster_level = 4\n",
        "frames_to_clusters_indices = {}\n",
        "clusters_indices= {}\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in frames_data_raw.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    if video_name not in frames_to_clusters_indices:\n",
        "      frames_to_clusters_indices[video_name] = []\n",
        "    try:\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, value[cluster_level-1]))\n",
        "      clusters_indices[value[cluster_level-1]]=0\n",
        "    except:\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, value[-1]))\n",
        "      clusters_indices[value[-1]]=0\n",
        "\n",
        "\n",
        "clusters_descriptors = []\n",
        "idx = 0\n",
        "for key, val in clusters_indices.items():\n",
        "  clusters_indices[key] = idx\n",
        "  a = clusters_data[key]\n",
        "  clusters_descriptors.append(clusters_data[key])\n",
        "  idx+=1\n",
        "clusters_descriptors = np.vstack(clusters_descriptors)\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "frames_data = {}\n",
        "for video_name, frames in frames_to_clusters_indices.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [clusters_indices[frame_data] for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    frames_data[video_name] = sorted_arrays\n",
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quwbj0hFhZ2G"
      },
      "outputs": [],
      "source": [
        "#Here, key is a video name and value is a list of cluster representatives of those clusters to which its frames are mapped\n",
        "\n",
        "clusters_rep_as_ground_truth_for_a_video = {}\n",
        "for video, frames in frames_data.items():\n",
        "  stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "  clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1oCIVPqhnWh"
      },
      "outputs": [],
      "source": [
        "# Define window size\n",
        "window_size = 2\n",
        "input_size = window_size*28 + 16*(window_size-1)\n",
        "output_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLCS32dkhzKg"
      },
      "outputs": [],
      "source": [
        "def extract_wav2vec_features_and_labels_from_csv(csv_path, scaler=None): #To get audio features (MFCC/Wav2Vec) of a video\n",
        "    df = pd.read_csv(csv_path, header=None)\n",
        "    features = df.iloc[:-1, :].values.astype(np.float32)\n",
        "    video_name = csv_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "    if scaler is not None:\n",
        "        features = scaler.transform(features)\n",
        "\n",
        "    return features, true_descriptors[video_name]#clusters_rep_as_ground_truth_for_a_video[video_name]#true_descriptors[video_name]\n",
        "\n",
        "def load_data_from_directory(directory_path):\n",
        "    return [os.path.join(directory_path, fname) for fname in os.listdir(directory_path) if fname.endswith(\".csv\")]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    features_padded = torch.nn.utils.rnn.pad_sequence(features, batch_first=True)\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return features_padded, labels_padded\n",
        "\n",
        "\n",
        "class SlidingWindowAudioDataset(Dataset):\n",
        "    def __init__(self, video_paths, num_clusters=500, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for audio features with sliding window implementation.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.data = []  # To store concatenated feature-label vectors\n",
        "        self.labels = []  # To store the corresponding output labels (Nth label)\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for video_path in self.video_paths:\n",
        "            features, labels = extract_wav2vec_features_and_labels_from_csv(video_path, self.scaler)\n",
        "\n",
        "            # Normalize features using the fitted scaler\n",
        "            if self.scaler:\n",
        "                features = self.scaler.transform(features)\n",
        "\n",
        "            num_rows = labels.shape[0]\n",
        "            # Generate sliding windows\n",
        "            for start in range(num_rows - self.window_size + 1):\n",
        "                end = start + self.window_size\n",
        "                # Select the current sliding window features and corresponding labels\n",
        "                window_features = features[start:end]\n",
        "                window_labels = labels[start:end]\n",
        "\n",
        "                # Concatenate the first (N-1) labels with the N features for input\n",
        "                # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "                input_vector = np.concatenate((window_features.flatten(), window_labels[:-1].flatten()))\n",
        "\n",
        "                # The Nth label is the target/output\n",
        "                target_label = window_labels[-1]\n",
        "                if input_vector.shape[0] == (self.window_size*28 + output_size*(self.window_size-1)):\n",
        "                  self.data.append(input_vector)\n",
        "                  self.labels.append(target_label)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.data[idx], dtype=torch.float32),  # The concatenated feature-label vector\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float32),  # The output label (Nth label)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcaP3rj0h0zY",
        "outputId": "18408f95-40b6-46bd-ff41-eaf67a849e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "mead_mfcc_path = \"/content/drive/MyDrive/MEAD_MFCC/\"\n",
        "# Load and prepare data\n",
        "dataset_path = \"DatasetForLSTM/\"+ Zipped_inside_folder\n",
        "\n",
        "# Update video paths\n",
        "# Go through each video name and update path of the csv --> wave to vec csvs\n",
        "all_video_paths = []\n",
        "for video in videos_list:\n",
        "  if 'M' in video: #For MEAD\n",
        "    all_video_paths.append(mead_mfcc_path+video+\".csv\")\n",
        "  else:\n",
        "    all_video_paths.append(\"DatasetForLSTM/\"+Zipped_inside_folder+\"/\"+video+\".csv\")\n",
        "\n",
        "train_video_paths, test_video_paths = train_test_split(all_video_paths, test_size=0.05, random_state=42)\n",
        "for video_path in train_video_paths:\n",
        "    features, _ = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "    scaler.partial_fit(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tydj-KPh8TQ"
      },
      "outputs": [],
      "source": [
        "#Batch Size\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the sliding window dataset\n",
        "train_dataset = SlidingWindowAudioDataset(train_video_paths, scaler=scaler, window_size=window_size)\n",
        "test_dataset = SlidingWindowAudioDataset(test_video_paths, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# DataLoader remains the same\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUoZ4n3Lh-aB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiOutputNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Multi-output neural network that outputs an N-dimensional vector.\n",
        "\n",
        "        Args:\n",
        "        - input_dim: Number of input features.\n",
        "        - output_dim: Size of the output vector (N).\n",
        "        \"\"\"\n",
        "        super(MultiOutputNN, self).__init__()\n",
        "\n",
        "        # Define the layers of the network\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64,128)\n",
        "        self.fc3 = nn.Linear(128,256)\n",
        "        self.fc4 = nn.Linear(256,128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 32)         # Second hidden layer\n",
        "        self.fc7 = nn.Linear(32, output_dim)  # Output layer (N-dimensional vector)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "        - output: Tensor of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = torch.relu(self.fc6(x))\n",
        "        output = self.fc7(x)         # Output layer (no activation)\n",
        "        return output\n",
        "\n",
        "model = MultiOutputNN(input_size, output_size).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wbFNUNLiCTK"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, device, alpha=0.5, beta=0.5):\n",
        "    model.train()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(train_loader)\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (features, labels) in enumerate(progress_bar):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        if outputs.shape[1] != labels.shape[1]:\n",
        "          outputs = outputs[:, :-1, :]\n",
        "\n",
        "        # Compute MSE loss\n",
        "        mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mse_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "        # Update progress bar with average batch loss so far\n",
        "        progress_bar.set_postfix({\n",
        "            \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "        })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "\n",
        "    print(f\"Training Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "    return avg_mse_loss\n",
        "\n",
        "def test(model, test_loader, device, alpha=0.5, beta=0.5):\n",
        "    model.eval()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(test_loader)\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, labels) in enumerate(progress_bar):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            if outputs.shape[1] != labels.shape[1]:\n",
        "              outputs = outputs[:, :-1, :]\n",
        "\n",
        "            # Compute MSE loss\n",
        "            mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "\n",
        "            cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "\n",
        "            # Update progress bar with average batch loss so far\n",
        "            progress_bar.set_postfix({\n",
        "\n",
        "                \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "\n",
        "            })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "    print(f\"Testing Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "\n",
        "    return avg_mse_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ou58sgCiDqZ",
        "outputId": "78edc5b3-f5e5-4077-ac53-e4f86f529b8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Optimizer, criterion, and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2pGdArdiFAq",
        "outputId": "d229e4f6-5ce4-4740-c5e9-c066385c0e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.016204583308075768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.01260197575330854\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.009060886052529468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.007379543385000355\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.008708735768401822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.008186126854080794\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.00850004341324102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.007687510872267225\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.008370268231900797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.008568111672772266\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "# model.load_state_dict(torch.load('drive/MyDrive/LSTM_Params/lstm_predicting_LP_descriptors.pth', weights_only=True))\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, device, alpha=1, beta=0.001)\n",
        "    test_loss = test(model, test_loader, device, alpha=1, beta=0.001)\n",
        "\n",
        "    #Step the scheduler with the test loss\n",
        "    # scheduler.step(test_loss)\n",
        "\n",
        "#     if test_loss < best_loss:\n",
        "#         best_loss = test_loss\n",
        "#         torch.save(model.state_dict(), 'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_best_model.pth')\n",
        "#         print(f\"New best model saved with loss: {best_loss:.2f}\")\n",
        "\n",
        "#     if epoch % 5 == 0:\n",
        "#         torch.save(model.state_dict(), f'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_epoch{epoch}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2kyFlPwiL__"
      },
      "source": [
        "# Testing wiht  first (Window_Size - 1) true descriptors per sliding window\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyC6qOaTiHKD"
      },
      "outputs": [],
      "source": [
        "#use predicted one for the next frame\n",
        "\n",
        "# Use it if not want to train\n",
        "class SlidingWindowPerVideoDatasetForAccuracy(Dataset):\n",
        "    def __init__(self, video_paths, frames_to_clusters_mapping, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for generating sliding windows for an entire video.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.frames_to_clusters_mapping = frames_to_clusters_mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        features, labels = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "        video_name = video_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "        # Normalize features using the scaler\n",
        "        if self.scaler:\n",
        "            features = self.scaler.transform(features)\n",
        "\n",
        "        num_rows = labels.shape[0]\n",
        "        sliding_windows_features = []\n",
        "        sliding_windows_labels = []\n",
        "        #Getting ground truth for the first Window_Size - 1 frames\n",
        "        for i in range(self.window_size-1):\n",
        "          sliding_windows_labels.append(self.frames_to_clusters_mapping[video_name][i])\n",
        "        # Generate sequential windows\n",
        "        for start in range(num_rows - self.window_size + 1):\n",
        "            end = start + self.window_size\n",
        "            # Select the current sliding window features and corresponding labels\n",
        "            window_features = features[start:end]\n",
        "            window_labels = labels[start:end]\n",
        "\n",
        "            # Concatenate the first (N-1) labels with the N features for input\n",
        "            # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "            input_vector = np.concatenate((window_features.flatten(), window_labels[:-1].flatten()))\n",
        "            sliding_windows_features.append(input_vector)\n",
        "            # The Nth label is the target/output\n",
        "            target_label = self.frames_to_clusters_mapping[video_name][end-1]\n",
        "            sliding_windows_labels.append(target_label)\n",
        "\n",
        "        return {\n",
        "            \"video\": video_name,\n",
        "            \"features\": [torch.tensor(w, dtype=torch.float32) for w in sliding_windows_features],\n",
        "            \"labels\": [w for w in sliding_windows_labels],\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc9t4XaRiLbL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Compute pairwise Euclidean distances\n",
        "pairwise_distances = squareform(pdist(clusters_descriptors, metric='euclidean'))\n",
        "\n",
        "# Set diagonal to infinity to ignore self-distances\n",
        "np.fill_diagonal(pairwise_distances, np.inf)\n",
        "\n",
        "\n",
        "# Find the indices of the top 5 nearest neighbors for each cluster\n",
        "cluster_nearest_neighbors = np.argsort(pairwise_distances, axis=1)[:, :1].tolist()\n",
        "\n",
        "\n",
        "# `pairwise_distances` is now a (500, 500) matrix of inter-cluster distances\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM1Yf_xYiQxT"
      },
      "outputs": [],
      "source": [
        "def sliding_window_per_video_collate_fn(batch):\n",
        "    video_name = batch[0]['video']\n",
        "    features = batch[0][\"features\"]  # List of sliding windows for each video\n",
        "    labels = batch[0][\"labels\"]  # Corresponding labels\n",
        "    return video_name, features, labels\n",
        "\n",
        "\n",
        "# Initialize the sliding window per video dataset\n",
        "train_dataset_ = SlidingWindowPerVideoDatasetForAccuracy(train_video_paths, frames_data, scaler=scaler, window_size=window_size)\n",
        "test_dataset_ = SlidingWindowPerVideoDatasetForAccuracy(test_video_paths,frames_data, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# Initialize the data loaders\n",
        "train_loader_ = DataLoader(train_dataset_, batch_size=1, shuffle=False, collate_fn=sliding_window_per_video_collate_fn)\n",
        "test_loader_ = DataLoader(test_dataset_, batch_size=1, shuffle=False, collate_fn=sliding_window_per_video_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHpyyJj4iUpM"
      },
      "outputs": [],
      "source": [
        "def get_predicted_labels(outputs, cluster_descriptors, device):\n",
        "    # Ensure cluster_descriptors is a torch tensor and move to device\n",
        "    cluster_descriptors = torch.tensor(cluster_descriptors, device=device, dtype=outputs.dtype)\n",
        "    #print(cluster_descriptors.shape)\n",
        "    # Initialize an empty list to store predicted labels for each frame\n",
        "    predicted_labels = []\n",
        "    #outputs = outputs.squeeze(0)\n",
        "    # Iterate over each frame descriptor in outputs\n",
        "    for frame_descriptor in outputs:\n",
        "        # Calculate Euclidean distances between the frame descriptor and each cluster descriptor\n",
        "        #print(frame_descriptor.shape)\n",
        "\n",
        "        distances = torch.norm(cluster_descriptors - frame_descriptor, dim=1)\n",
        "\n",
        "        # Find the index of the minimum distance (i.e., closest cluster descriptor)\n",
        "        predicted_label = torch.argmin(distances)\n",
        "        predicted_labels.append(predicted_label.item())\n",
        "\n",
        "    # Convert predicted labels list to a tensor on the same device\n",
        "    return torch.tensor(predicted_labels, device=device)\n",
        "\n",
        "def evaluate_video_predictions(outputs, cluster_descriptors, actual_labels_names, device, cluster_nearest_neighbors):\n",
        "    # Get predicted labels based on Euclidean distance comparison\n",
        "    predicted_labels = get_predicted_labels(outputs, cluster_descriptors, device)\n",
        "    # Get the actual labels for the frames from frames_data and convert to torch tensor on the same device\n",
        "    #print(frames_data)\n",
        "    actual_labels = torch.tensor(actual_labels_names, device=device)\n",
        "    # Initialize a counter for matches\n",
        "    matches = 0\n",
        "    total_labels = actual_labels.size(0)\n",
        "    # Iterate through each label and check for matches or nearest neighbor matches\n",
        "    for i in range(total_labels):\n",
        "        actual_label = actual_labels[i].item()\n",
        "        predicted_label = predicted_labels[i].item()\n",
        "\n",
        "        # Check if predicted label is a match or a match with the nearest neighbor\n",
        "        if predicted_label == actual_label or  predicted_label in cluster_nearest_neighbors[actual_label]:\n",
        "            matches += 1\n",
        "\n",
        "    return predicted_labels.cpu().numpy(), matches, total_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o6v_IVhiWOE",
        "outputId": "8318ee07-b041-43da-88b9-e50b4b31577e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy without Window:  23.651835571143117 %\n",
            "Train Accuracy with Window:  24.269241845867125 %\n",
            "Test Accuracy without Window:  23.357507000789834 %\n",
            "Test Accuracy with Window:  23.991953429583607 %\n",
            "Accuracy without Window (Test + Training):  23.637417263675513 %\n",
            "Accuracy with Window (Test + Training):  24.255655794829213 %\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "matched = 0\n",
        "total = 0\n",
        "\n",
        "matched_with_window = 0\n",
        "total_with_window = 0\n",
        "inc = window_size-1\n",
        "for video_name, features_list, actual_labels in train_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        labels_for_video.extend(predicted_labels.tolist())\n",
        "        matched+=matches\n",
        "        total += total_labels\n",
        "        matched_with_window += matches\n",
        "        total_with_window +=total_labels\n",
        "    matched_with_window += inc\n",
        "    total_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Train Accuracy without Window: \", (matched * 100)/total, \"%\")\n",
        "print(\"Train Accuracy with Window: \", (matched_with_window * 100)/total_with_window, \"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "matched_test_with_window = 0\n",
        "total_test_with_window = 0\n",
        "for video_name, features_list, actual_labels in test_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        labels_for_video.extend(predicted_labels.tolist())\n",
        "        matched_test+=matches\n",
        "        total_test += total_labels\n",
        "        matched_test_with_window += matches\n",
        "        total_test_with_window += total_labels\n",
        "    matched_test_with_window += inc\n",
        "    total_test_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Test Accuracy without Window: \", (matched_test*100)/total_test, \"%\")\n",
        "print(\"Test Accuracy with Window: \", (matched_test_with_window*100)/total_test_with_window, \"%\")\n",
        "\n",
        "print(\"Accuracy without Window (Test + Training): \", ((matched_test+matched)*100)/(total_test+total), \"%\")\n",
        "print(\"Accuracy with Window (Test + Training): \", ((matched_test_with_window+matched_with_window)*100)/(total_test_with_window+total_with_window), \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqrwg5Utidqu"
      },
      "source": [
        "# Testing wiht only first (Window_Size - 1) true descriptors per video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v2z1J3bWia6U"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(Dataset):\n",
        "    def __init__(self, video_paths, frames_to_clusters_mapping, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for generating sliding windows for an entire video.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.frames_to_clusters_mapping = frames_to_clusters_mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        features, labels = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "        video_name = video_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "        # Normalize features using the scaler\n",
        "        if self.scaler:\n",
        "            features = self.scaler.transform(features)\n",
        "\n",
        "        num_rows = labels.shape[0]\n",
        "        sliding_windows_features = []\n",
        "        sliding_windows_labels = []\n",
        "        actual_descriptors = labels[0:self.window_size-1]\n",
        "\n",
        "        #Getting ground truth for the first Window_Size - 1 frames\n",
        "        for i in range(self.window_size-1):\n",
        "          sliding_windows_labels.append(self.frames_to_clusters_mapping[video_name][i])\n",
        "        # Generate sequential windows\n",
        "        for start in range(num_rows - self.window_size + 1):\n",
        "            end = start + self.window_size\n",
        "            # Select the current sliding window features and corresponding labels\n",
        "            window_features = features[start:end]\n",
        "\n",
        "\n",
        "            # Concatenate the first (N-1) labels with the N features for input\n",
        "            # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "            input_vector = window_features.flatten()\n",
        "            sliding_windows_features.append(input_vector)\n",
        "            # The Nth label is the target/output\n",
        "            target_label = self.frames_to_clusters_mapping[video_name][end-1]\n",
        "            sliding_windows_labels.append(target_label)\n",
        "\n",
        "        return {\n",
        "            \"video\": video_name,\n",
        "            \"features\": [torch.tensor(w, dtype=torch.float32) for w in sliding_windows_features],\n",
        "            \"labels\": [w for w in sliding_windows_labels],\n",
        "            \"descriptors\": actual_descriptors,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WzrTN6KpifBO"
      },
      "outputs": [],
      "source": [
        "def sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn(batch):\n",
        "    video_name = batch[0]['video']\n",
        "    features = batch[0][\"features\"]  # List of sliding windows for each video\n",
        "    labels = batch[0][\"labels\"]  # Corresponding labels\n",
        "    actual_descriptors = batch[0][\"descriptors\"]\n",
        "    return video_name, features, labels, actual_descriptors\n",
        "\n",
        "\n",
        "# Initialize the sliding window per video dataset\n",
        "train_dataset__ = SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(train_video_paths, frames_data, scaler=scaler, window_size=window_size)\n",
        "test_dataset__ = SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(test_video_paths,frames_data, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# Initialize the data loaders\n",
        "train_loader__ = DataLoader(train_dataset__, batch_size=1, shuffle=False, collate_fn=sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn)\n",
        "test_loader__ = DataLoader(test_dataset__, batch_size=1, shuffle=False, collate_fn=sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qhLc5LgigmZ"
      },
      "outputs": [],
      "source": [
        "def updateTrueDescriptorsList(actual_descriptors, window_size, new_predicted_descriptor):\n",
        "    # Roll the array by 1 (shifts all rows by one, and the last row will be empty)\\\n",
        "    if window_size != 2:\n",
        "      actual_descriptors = np.roll(actual_descriptors, shift=-1, axis=0)\n",
        "\n",
        "      # Add the new predicted descriptor at the last row\n",
        "      actual_descriptors[window_size - 2] = new_predicted_descriptor.detach().cpu().numpy()\n",
        "    else:\n",
        "      actual_descriptors = new_predicted_descriptor.detach().cpu().numpy()\n",
        "    return actual_descriptors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "THd2pKxCiiM-",
        "outputId": "b60f7b82-f9f8-4e12-a6cc-24cfe36af591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average MSE Train: 0.331453685944762\n",
            "Average MSE Test: 0.24782005577918984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MSE calculate MSE when we use predicted descriptors\n",
        "train_mse = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in train_loader__:\n",
        "    length = len(features_list)\n",
        "    #print(length)\n",
        "    predicted_descriptors = []\n",
        "    #print(true_descriptors[video_name].shape)\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        #print(outputs.shape)\n",
        "        predicted_descriptors.append(outputs.detach().cpu().numpy())\n",
        "    predicted_descriptors = np.vstack(predicted_descriptors)\n",
        "    predicted_descriptors = torch.tensor(predicted_descriptors, dtype=torch.float32).to(device)\n",
        "    actual_descriptors_ = true_descriptors[video_name][1:predicted_descriptors.shape[0]+1]\n",
        "    actual_descriptors_ = torch.tensor(actual_descriptors_, dtype=torch.float32).to(device)\n",
        "    mse_loss = F.mse_loss(predicted_descriptors, actual_descriptors_, reduction = 'mean')\n",
        "    train_mse += mse_loss.item()\n",
        "print(\"Average MSE Train:\", train_mse/len(train_loader__))\n",
        "\n",
        "\n",
        "# test for same\n",
        "test_mse = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in test_loader__:\n",
        "    length = len(features_list)\n",
        "    #print(length)\n",
        "    predicted_descriptors = []\n",
        "    #print(true_descriptors[video_name].shape)\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        #print(outputs.shape)\n",
        "        predicted_descriptors.append(outputs.detach().cpu().numpy())\n",
        "    predicted_descriptors = np.vstack(predicted_descriptors)\n",
        "    predicted_descriptors = torch.tensor(predicted_descriptors, dtype=torch.float32).to(device)\n",
        "    actual_descriptors_ = true_descriptors[video_name][1:predicted_descriptors.shape[0]+1]\n",
        "    actual_descriptors_ = torch.tensor(actual_descriptors_, dtype=torch.float32).to(device)\n",
        "    mse_loss = F.mse_loss(predicted_descriptors, actual_descriptors_, reduction = 'mean')\n",
        "    test_mse += mse_loss.item()\n",
        "print(\"Average MSE Test:\", test_mse/len(test_loader__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8RotBYRijeB",
        "outputId": "4f81cbd3-f8a4-4748-9805-9be14f8d9a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy without Window:  42.847407601926605 %\n",
            "Train Accuracy with Window:  43.3095846487133 %\n",
            "Test Accuracy without Window:  43.18230774754075 %\n",
            "Test Accuracy with Window:  43.65264450892778 %\n",
            "Accuracy without Window (Test + Training):  42.863813393809515 %\n",
            "Accuracy with Window (Test + Training):  43.326393244010575 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Give accuracy instead of MSE when we use predicted descriptors\n",
        "results = {}\n",
        "matched = 0\n",
        "total = 0\n",
        "\n",
        "matched_with_window = 0\n",
        "total_with_window = 0\n",
        "inc = window_size-1\n",
        "for video_name, features_list, actual_labels, actual_descriptors in train_loader__:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        labels_for_video.extend(predicted_labels)\n",
        "        matched+=matches\n",
        "        total += total_labels\n",
        "        matched_with_window += matches\n",
        "        total_with_window +=total_labels\n",
        "    matched_with_window += inc\n",
        "    total_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Train Accuracy without Window: \", (matched * 100)/total, \"%\")\n",
        "print(\"Train Accuracy with Window: \", (matched_with_window * 100)/total_with_window, \"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "matched_test_with_window = 0\n",
        "total_test_with_window = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in test_loader__:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        labels_for_video.extend(predicted_labels)\n",
        "        matched_test+=matches\n",
        "        total_test += total_labels\n",
        "        matched_test_with_window += matches\n",
        "        total_test_with_window += total_labels\n",
        "    matched_test_with_window += inc\n",
        "    total_test_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Test Accuracy without Window: \", (matched_test*100)/total_test, \"%\")\n",
        "print(\"Test Accuracy with Window: \", (matched_test_with_window*100)/total_test_with_window, \"%\")\n",
        "\n",
        "print(\"Accuracy without Window (Test + Training): \", ((matched_test+matched)*100)/(total_test+total), \"%\")\n",
        "print(\"Accuracy with Window (Test + Training): \", ((matched_test_with_window+matched_with_window)*100)/(total_test_with_window+total_with_window), \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0DiaHeNilx3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}