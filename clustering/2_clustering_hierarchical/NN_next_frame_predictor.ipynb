{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hashawaji/portrait-video-synthesis/blob/main/clustering/2_clustering_hierarchical/NN_next_frame_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZadBcLGb4Sgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9ba4ac-080c-46dc-e40d-ede8f6751dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P8ospWlK4ZUC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YQepxgy34a2Y"
      },
      "outputs": [],
      "source": [
        "#loading MFCC features of RAVDESS dataset\n",
        "datasetPath = 'DatasetForLSTM'\n",
        "Zipped_inside_folder = 'RAVDESS_MFCC'\n",
        "with zipfile.ZipFile('drive/MyDrive/'+Zipped_inside_folder+'.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(datasetPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87E9b5Jp4cSA",
        "outputId": "2b3ab968-2f4a-42ce-eb9b-ba38d0fc9993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9282\n",
            "287880\n",
            "868263\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "#getting descriptors of frames of each video and storing this information in a dictionary (true_descriptors) where key is video name and value is a list of descriptors of its frames.\n",
        "\n",
        "clusters_info = 'Sawaiz_2/pkl_for_lstm_encoded/17_53_50_800'\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/live_portrait_descriptors_all_encoder.pkl'\n",
        "\n",
        "# Open the file in binary read mode and load the data\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "video_dict = defaultdict(list)\n",
        "\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in data.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    video_dict[video_name].append((frame_number, value))\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "final_video_dict = {}\n",
        "for video_name, frames in video_dict.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [frame_data for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    final_video_dict[video_name] = np.vstack(sorted_arrays)\n",
        "true_descriptors = final_video_dict\n",
        "videos_list = list(true_descriptors.keys())\n",
        "print(len(videos_list))\n",
        "print(ravdes)\n",
        "print(meads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ygs0sXq5JCN"
      },
      "source": [
        "# Getting Clusters Descriptors and Frames Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt4wPwkW4vb2"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/sub_cluster_average_vectors.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/hierarchical_clusters_frame_to_cluster.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SK5uys7ryRzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS0UvzxOaJh4"
      },
      "outputs": [],
      "source": [
        "k = list(frames_data.keys())[0]\n",
        "print(frames_data[k])\n",
        "#print(list(frames_data.keys())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWbVtVY6aqH4"
      },
      "outputs": [],
      "source": [
        "frames_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymv6WcBz4zto"
      },
      "outputs": [],
      "source": [
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames..\n",
        "for key, value in frames_data.items():\n",
        "    sorted_value = sorted(value, key=lambda x: int(x[0].split('_')[-1].split('.')[0]) if '_' in x[0] else int(x[0].split('.')[0]))\n",
        "    frames_data[key] = sorted_value\n",
        "for key, val in frames_data.items():\n",
        "    frames_data[key] = [ x[1] for x in val]\n",
        "\n",
        "frames_data_new = {}\n",
        "for key, val in frames_data.items():\n",
        "    if 'M' in key:\n",
        "      parts = key.split(\"/\")\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frames_data_new[video_name] = val\n",
        "    else:\n",
        "      frames_data_new[key] = val\n",
        "frames_data = frames_data_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRtF32dx41b8"
      },
      "outputs": [],
      "source": [
        "number_of_clusters = 800 # Updated\n",
        "stacked_descriptors = [clusters_data[val] for val in range(number_of_clusters)]\n",
        "clusters_descriptors = np.vstack(stacked_descriptors) #A numpy array of cluster decriptors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzFoXIoW5Rpi"
      },
      "source": [
        "Clusters descriptors as Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F9me0vC43di"
      },
      "outputs": [],
      "source": [
        "#Here, key is a video name and value is a list of cluster representatives of those clusters to which its frames are mapped\n",
        "clusters_rep_as_ground_truth_for_a_video = {}\n",
        "for video, frames in frames_data.items():\n",
        "  stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "  clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLZeOrffAYyn"
      },
      "source": [
        "# For Hierarchical Clustering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/cluster_rep_level2.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/frame_to_cluster_level2.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data_raw = pickle.load(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "9-yXsmLp0q2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(list(clusters_data.keys())[0])\n",
        "print(clusters_data['Cluster_0.0'])\n",
        "print(frames_data_raw['M009/front/happy/level_3/029/frame_0012.jpg'])\n",
        "#print(list(frames_data_raw.keys())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFtPqCzg0rvg",
        "outputId": "e6359098-e224-47f9-f4a1-ef695f6244f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.45084262 0.         0.         2.9121113  0.         4.6745005\n",
            " 2.1946874  5.64312    1.8479906  3.4048476  1.5221575  1.3931155\n",
            " 0.34884953 0.6637747  2.8054526  5.492489  ]\n",
            "Cluster_1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "waIMjkyJAcSC"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/cluster_rep_level2.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    clusters_data = pickle.load(file)\n",
        "\n",
        "# Actual labels for the LP\n",
        "file_path = '/content/drive/MyDrive/'+clusters_info+'/frame_to_cluster_level2.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    frames_data_raw = pickle.load(file)\n",
        "\n",
        "cluster_level = 4\n",
        "frames_to_clusters_indices = {}\n",
        "clusters_indices= {}\n",
        "# Populate the video_dict with frame arrays in order\n",
        "for key, value in frames_data_raw.items():\n",
        "    # Split the key to extract video name and frame number\n",
        "    parts = key.split('/')\n",
        "    if 'M' not in key: #For Ravdess data\n",
        "      video_name = parts[1]  # Extracts '02-01-01-01-02-02-16'\n",
        "      frame_number = int(parts[2].split('.')[0])  # Extracts frame number as an integer (e.g., 1)\n",
        "\n",
        "    else: #for MEAD\n",
        "      video_name = parts[0] + \"__\" + parts[2] + \"__\" + parts[3] + \"__\" + parts[4]\n",
        "      frame_number = int(parts[-1].split(\".\")[0].split(\"_\")[-1])\n",
        "    # Append the frame array to the respective video entry in the dictionary\n",
        "    if video_name not in frames_to_clusters_indices:\n",
        "      frames_to_clusters_indices[video_name] = []\n",
        "    try:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "    except:\n",
        "      cluster_name = value\n",
        "      frames_to_clusters_indices[video_name].append((frame_number, cluster_name))\n",
        "      clusters_indices[cluster_name]=0\n",
        "\n",
        "\n",
        "\n",
        "clusters_descriptors = []\n",
        "idx = 0\n",
        "for key, val in clusters_indices.items():\n",
        "  clusters_indices[key] = idx\n",
        "  #print(key)\n",
        "  clusters_descriptors.append(clusters_data[key])\n",
        "  idx+=1\n",
        "clusters_descriptors = np.vstack(clusters_descriptors)\n",
        "\n",
        "# Sort frames for each video by frame number and concatenate them into a single array\n",
        "frames_data = {}\n",
        "for video_name, frames in frames_to_clusters_indices.items():\n",
        "    # Sort frames by frame number to ensure the order is correct\n",
        "    sorted_frames = sorted(frames, key=lambda x: x[0])\n",
        "    # Extract only the frame data, discarding the frame numbers\n",
        "    sorted_arrays = [clusters_indices[frame_data] for _, frame_data in sorted_frames]\n",
        "    # Concatenate all frames into a single numpy array\n",
        "    frames_data[video_name] = sorted_arrays\n",
        "\n",
        "#Here, frames_data is a dictionary where key is video name and value is list of cluster ids of its frames.\n",
        "\n",
        "clusters_rep_as_ground_truth_for_a_video = {}\n",
        "for video, frames in frames_data.items():\n",
        "  stacked_clusters_rep = [clusters_descriptors[val] for val in frames]\n",
        "  clusters_rep_as_ground_truth_for_a_video[video] = np.vstack(stacked_clusters_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOwptD4R9m6q"
      },
      "outputs": [],
      "source": [
        "# Define window size\n",
        "window_size = 2\n",
        "input_size = window_size*28 + 16*(window_size-1)\n",
        "output_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM833vUf-IE8"
      },
      "source": [
        "# Define functions for dataset definition and MFCC csv reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvd0nUtv4dz4"
      },
      "outputs": [],
      "source": [
        "def extract_wav2vec_features_and_labels_from_csv(csv_path, scaler=None): #To get audio features (MFCC/Wav2Vec) of a video\n",
        "    df = pd.read_csv(csv_path, header=None)\n",
        "    features = df.iloc[:-1, :].values.astype(np.float32)\n",
        "    video_name = csv_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "    if scaler is not None:\n",
        "        features = scaler.transform(features)\n",
        "\n",
        "    return features, true_descriptors[video_name]#clusters_rep_as_ground_truth_for_a_video[video_name]#true_descriptors[video_name]\n",
        "\n",
        "def load_data_from_directory(directory_path):\n",
        "    return [os.path.join(directory_path, fname) for fname in os.listdir(directory_path) if fname.endswith(\".csv\")]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    features_padded = torch.nn.utils.rnn.pad_sequence(features, batch_first=True)\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return features_padded, labels_padded\n",
        "\n",
        "\n",
        "class SlidingWindowAudioDataset(Dataset):\n",
        "    def __init__(self, video_paths, num_clusters=500, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for audio features with sliding window implementation.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.data = []  # To store concatenated feature-label vectors\n",
        "        self.labels = []  # To store the corresponding output labels (Nth label)\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for video_path in self.video_paths:\n",
        "            features, labels = extract_wav2vec_features_and_labels_from_csv(video_path, self.scaler)\n",
        "\n",
        "            # Normalize features using the fitted scaler\n",
        "            if self.scaler:\n",
        "                features = self.scaler.transform(features)\n",
        "\n",
        "            num_rows = labels.shape[0]\n",
        "            # Generate sliding windows\n",
        "            for start in range(num_rows - self.window_size + 1):\n",
        "                end = start + self.window_size\n",
        "                # Select the current sliding window features and corresponding labels\n",
        "                window_features = features[start:end]\n",
        "                window_labels = labels[start:end]\n",
        "\n",
        "                # Concatenate the first (N-1) labels with the N features for input\n",
        "                # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "                input_vector = np.concatenate((window_features.flatten(), window_labels[:-1].flatten()))\n",
        "\n",
        "                # The Nth label is the target/output\n",
        "                target_label = window_labels[-1]\n",
        "                if input_vector.shape[0] == (self.window_size*28 + output_size*(self.window_size-1)):\n",
        "                  self.data.append(input_vector)\n",
        "                  self.labels.append(target_label)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.data[idx], dtype=torch.float32),  # The concatenated feature-label vector\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float32),  # The output label (Nth label)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL7jwIls-E_c"
      },
      "source": [
        "# Creating DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ernG0zxy43Ks",
        "outputId": "1bc8dfd2-f18a-40ec-cf3f-6ddd0295f80b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Scaler loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "import joblib\n",
        "#scaler = MinMaxScaler()\n",
        "# Save the trained scaler to a file\n",
        "scaler_filename = \"/content/drive/MyDrive/Sawaiz_2/results_pkl_videos/scaler_MEAD2.pkl\"\n",
        "# joblib.dump(scaler, scaler_filename)\n",
        "# print(f\"Scaler saved to {scaler_filename}\")\n",
        "# Load the scaler from the file\n",
        "scaler = joblib.load(scaler_filename)\n",
        "print(\"Scaler loaded successfully!\")\n",
        "\n",
        "mead_mfcc_path = \"/content/drive/MyDrive/MEAD_MFCC/\"\n",
        "# Load and prepare data\n",
        "dataset_path = \"DatasetForLSTM/\"+ Zipped_inside_folder\n",
        "\n",
        "# Update video paths\n",
        "# Go through each video name and update path of the csv --> wave to vec csvs\n",
        "all_video_paths = []\n",
        "for video in videos_list:\n",
        "  if 'M' in video: #For MEAD\n",
        "    all_video_paths.append(mead_mfcc_path+video+\".csv\")\n",
        "  else:\n",
        "    all_video_paths.append(\"DatasetForLSTM/\"+Zipped_inside_folder+\"/\"+video+\".csv\")\n",
        "\n",
        "train_video_paths, test_video_paths = train_test_split(all_video_paths, test_size=0.05, random_state=42)\n",
        "# for video_path in train_video_paths:\n",
        "#     features, _ = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "#     scaler.partial_fit(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65UJldcg45qr"
      },
      "outputs": [],
      "source": [
        "#Batch Size\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the sliding window dataset\n",
        "train_dataset = SlidingWindowAudioDataset(train_video_paths, scaler=scaler, window_size=window_size)\n",
        "test_dataset = SlidingWindowAudioDataset(test_video_paths, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# DataLoader remains the same\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEcq1_1EEjLS",
        "outputId": "e0707171-46bb-4b50-8f5d-f21840bde4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33797\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YahFXIG3-Bef"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUMOi_RO5dSG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiOutputNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Multi-output neural network that outputs an N-dimensional vector.\n",
        "\n",
        "        Args:\n",
        "        - input_dim: Number of input features.\n",
        "        - output_dim: Size of the output vector (N).\n",
        "        \"\"\"\n",
        "        super(MultiOutputNN, self).__init__()\n",
        "\n",
        "        # Define the layers of the network\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64,128)\n",
        "        self.fc3 = nn.Linear(128,256)\n",
        "        self.fc4 = nn.Linear(256,128)\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.fc6 = nn.Linear(64, 32)         # Second hidden layer\n",
        "        self.fc7 = nn.Linear(32, output_dim)  # Output layer (N-dimensional vector)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "        - x: Input tensor of shape (batch_size, input_dim)\n",
        "\n",
        "        Returns:\n",
        "        - output: Tensor of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = torch.relu(self.fc6(x))\n",
        "        output = self.fc7(x)         # Output layer (no activation)\n",
        "        return output\n",
        "\n",
        "model = MultiOutputNN(input_size, output_size).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agbyMO7r98Gx"
      },
      "source": [
        "# MSE ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F-FN2FC6WBS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device, alpha=0.5, beta=0.5):\n",
        "    model.train()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(train_loader)\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (features, labels) in enumerate(progress_bar):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features)\n",
        "        if outputs.shape[1] != labels.shape[1]:\n",
        "          outputs = outputs[:, :-1, :]\n",
        "\n",
        "        # Compute MSE loss\n",
        "        mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mse_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "        # Update progress bar with average batch loss so far\n",
        "        progress_bar.set_postfix({\n",
        "\n",
        "            \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "        })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "\n",
        "    print(f\"Training Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "    return avg_mse_loss\n",
        "\n",
        "def test(model, test_loader, device, alpha=0.5, beta=0.5):\n",
        "    model.eval()\n",
        "    cumulative_mse_loss = 0  # MSE loss for epoch\n",
        "    num_batches = len(test_loader)\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, labels) in enumerate(progress_bar):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            if outputs.shape[1] != labels.shape[1]:\n",
        "              outputs = outputs[:, :-1, :]\n",
        "\n",
        "            # Compute MSE loss\n",
        "            mse_loss = F.mse_loss(outputs, labels, reduction = 'mean')\n",
        "\n",
        "\n",
        "            cumulative_mse_loss += mse_loss.item()\n",
        "\n",
        "\n",
        "            # Update progress bar with average batch loss so far\n",
        "            progress_bar.set_postfix({\n",
        "\n",
        "                \"Average MSE\": cumulative_mse_loss / (i + 1)\n",
        "\n",
        "            })\n",
        "\n",
        "    # Calculate average loss for epoch\n",
        "    avg_mse_loss = cumulative_mse_loss / num_batches\n",
        "    print(f\"Testing Epoch Average MSE Loss: {avg_mse_loss}\", sep=\" \")\n",
        "\n",
        "    return avg_mse_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OvXM3D_6Yab",
        "outputId": "56dd0a2f-3aa2-4581-f53c-307d1741ff4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Optimizer, criterion, and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0l0p-bH6aIC",
        "outputId": "a79b18d8-0336-4d41-a951-3e5b255f7487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.01591370246590757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.00772313422590707\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.009008498424267052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.007837154458220492\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.008693807750061991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.00761877074604319\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.008528676680187664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.006913910027741017\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch Average MSE Loss: 0.008428694348244456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Epoch Average MSE Loss: 0.007418291072485811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "# model.load_state_dict(torch.load('drive/MyDrive/LSTM_Params/lstm_predicting_LP_descriptors.pth', weights_only=True))\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, device, alpha=1, beta=0.001)\n",
        "    test_loss = test(model, test_loader, device, alpha=1, beta=0.001)\n",
        "\n",
        "    #Step the scheduler with the test loss\n",
        "    # scheduler.step(test_loss)\n",
        "\n",
        "#     if test_loss < best_loss:\n",
        "#         best_loss = test_loss\n",
        "#         torch.save(model.state_dict(), 'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_best_model.pth')\n",
        "#         print(f\"New best model saved with loss: {best_loss:.2f}\")\n",
        "\n",
        "#     if epoch % 5 == 0:\n",
        "#         torch.save(model.state_dict(), f'drive/MyDrive/LSTM_Params/lstm_2_predicting_descriptors_epoch{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imcQYCBXr3Ec",
        "outputId": "6a73298c-85b6-458f-bcb6-cad96cd2c02b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 3.2527e-04, -5.1126e-07,  3.2076e-06,  2.9842e-06,  1.9545e-02,\n",
            "        -2.5333e-03,  4.0983e-06, -5.8326e-05, -6.4493e-05, -1.1866e-02,\n",
            "         1.6763e-05, -4.1939e-05,  1.0166e-04, -3.3896e-08, -5.6227e-08,\n",
            "         3.5003e-04], device='cuda:0', grad_fn=<SubBackward0>)\n",
            "tensor([ 3.2527e-04, -5.1126e-07,  3.2076e-06,  2.9842e-06,  4.5585e-01,\n",
            "         1.1819e+00,  4.0983e-06, -5.8326e-05, -6.4493e-05,  5.1973e-01,\n",
            "         1.6763e-05, -4.1939e-05,  1.0166e-04, -3.3896e-08, -5.6227e-08,\n",
            "         3.5003e-04], device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.4363, 1.1844, 0.0000, 0.0000, 0.0000,\n",
            "        0.5316, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbyqMjG3L89i"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), '/content/drive/MyDrive/Sawaiz_2/saved_models/NN_nextframePred__Window2_MEAD2_GT:Frames_800_clusters.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJYHZO6R93Qh"
      },
      "source": [
        "# Testing wiht  first (Window_Size - 1) true descriptors per sliding window\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpTkHU4Q6dvq"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowPerVideoDatasetForAccuracy(Dataset):\n",
        "    def __init__(self, video_paths, frames_to_clusters_mapping, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for generating sliding windows for an entire video.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.frames_to_clusters_mapping = frames_to_clusters_mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        features, labels = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "        video_name = video_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "        # Normalize features using the scaler\n",
        "        if self.scaler:\n",
        "            features = self.scaler.transform(features)\n",
        "\n",
        "        num_rows = labels.shape[0]\n",
        "        sliding_windows_features = []\n",
        "        sliding_windows_labels = []\n",
        "        #Getting ground truth for the first Window_Size - 1 frames\n",
        "        for i in range(self.window_size-1):\n",
        "          sliding_windows_labels.append(self.frames_to_clusters_mapping[video_name][i])\n",
        "        # Generate sequential windows\n",
        "        for start in range(num_rows - self.window_size + 1):\n",
        "            end = start + self.window_size\n",
        "            # Select the current sliding window features and corresponding labels\n",
        "            window_features = features[start:end]\n",
        "            window_labels = labels[start:end]\n",
        "\n",
        "            # Concatenate the first (N-1) labels with the N features for input\n",
        "            # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "            input_vector = np.concatenate((window_features.flatten(), window_labels[:-1].flatten()))\n",
        "            sliding_windows_features.append(input_vector)\n",
        "            # The Nth label is the target/output\n",
        "            target_label = self.frames_to_clusters_mapping[video_name][end-1]\n",
        "            sliding_windows_labels.append(target_label)\n",
        "\n",
        "        return {\n",
        "            \"video\": video_name,\n",
        "            \"features\": [torch.tensor(w, dtype=torch.float32) for w in sliding_windows_features],\n",
        "            \"labels\": [w for w in sliding_windows_labels],\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ-hwT309jcI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Compute pairwise Euclidean distances\n",
        "pairwise_distances = squareform(pdist(clusters_descriptors, metric='euclidean'))\n",
        "\n",
        "# Set diagonal to infinity to ignore self-distances\n",
        "np.fill_diagonal(pairwise_distances, np.inf)\n",
        "\n",
        "\n",
        "# Find the indices of the top 5 nearest neighbors for each cluster\n",
        "cluster_nearest_neighbors = np.argsort(pairwise_distances, axis=1)[:, :1].tolist()\n",
        "\n",
        "\n",
        "# `pairwise_distances` is now a (500, 500) matrix of inter-cluster distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA79AGhq9l59"
      },
      "outputs": [],
      "source": [
        "def sliding_window_per_video_collate_fn(batch):\n",
        "    video_name = batch[0]['video']\n",
        "    features = batch[0][\"features\"]  # List of sliding windows for each video\n",
        "    labels = batch[0][\"labels\"]  # Corresponding labels\n",
        "    return video_name, features, labels\n",
        "\n",
        "\n",
        "# Initialize the sliding window per video dataset\n",
        "train_dataset_ = SlidingWindowPerVideoDatasetForAccuracy(train_video_paths, frames_data, scaler=scaler, window_size=window_size)\n",
        "test_dataset_ = SlidingWindowPerVideoDatasetForAccuracy(test_video_paths,frames_data, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# Initialize the data loaders\n",
        "train_loader_ = DataLoader(train_dataset_, batch_size=1, shuffle=False, collate_fn=sliding_window_per_video_collate_fn)\n",
        "test_loader_ = DataLoader(test_dataset_, batch_size=1, shuffle=False, collate_fn=sliding_window_per_video_collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJfGDRBQ9slY"
      },
      "source": [
        "# Predicted Descriptor Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9d9DnSv9qD4"
      },
      "outputs": [],
      "source": [
        "def get_predicted_labels(outputs, cluster_descriptors, device):\n",
        "    # Ensure cluster_descriptors is a torch tensor and move to device\n",
        "    cluster_descriptors = torch.tensor(cluster_descriptors, device=device, dtype=outputs.dtype)\n",
        "    #print(cluster_descriptors.shape)\n",
        "    # Initialize an empty list to store predicted labels for each frame\n",
        "    predicted_labels = []\n",
        "    #outputs = outputs.squeeze(0)\n",
        "    # Iterate over each frame descriptor in outputs\n",
        "    for frame_descriptor in outputs:\n",
        "        # Calculate Euclidean distances between the frame descriptor and each cluster descriptor\n",
        "        #print(frame_descriptor.shape)\n",
        "\n",
        "        distances = torch.norm(cluster_descriptors - frame_descriptor, dim=1)\n",
        "\n",
        "        # Find the index of the minimum distance (i.e., closest cluster descriptor)\n",
        "        predicted_label = torch.argmin(distances)\n",
        "        predicted_labels.append(predicted_label.item())\n",
        "\n",
        "    # Convert predicted labels list to a tensor on the same device\n",
        "    return torch.tensor(predicted_labels, device=device)\n",
        "\n",
        "def evaluate_video_predictions(outputs, cluster_descriptors, actual_labels_names, device, cluster_nearest_neighbors):\n",
        "    # Get predicted labels based on Euclidean distance comparison\n",
        "    predicted_labels = get_predicted_labels(outputs, cluster_descriptors, device)\n",
        "    # Get the actual labels for the frames from frames_data and convert to torch tensor on the same device\n",
        "    #print(frames_data)\n",
        "    actual_labels = torch.tensor(actual_labels_names, device=device)\n",
        "    # Initialize a counter for matches\n",
        "    matches = 0\n",
        "    total_labels = actual_labels.size(0)\n",
        "    # Iterate through each label and check for matches or nearest neighbor matches\n",
        "    for i in range(total_labels):\n",
        "        actual_label = actual_labels[i].item()\n",
        "        predicted_label = predicted_labels[i].item()\n",
        "\n",
        "        # Check if predicted label is a match or a match with the nearest neighbor\n",
        "        if predicted_label == actual_label or  predicted_label in cluster_nearest_neighbors[actual_label]:\n",
        "            matches += 1\n",
        "\n",
        "    return predicted_labels.cpu().numpy(), matches, total_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmzotMAccdfH",
        "outputId": "e4916d7b-26cf-4337-84ff-a7dc1d15e7a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiOutputNN(\n",
              "  (fc1): Linear(in_features=72, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc6): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc7): Linear(in_features=32, out_features=16, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Sawaiz_2/saved_models/NN_nextframePred__Window2_MEAD2_GT:Frames_800_clusters.pth', weights_only=True)) #Reload path add\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVTXuyNC9rrI",
        "outputId": "c380facb-1ae0-4772-b997-efb996d25fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy without Window:  0.6798034931469483 %\n",
            "Train Accuracy with Window:  1.4829781731000289 %\n",
            "Test Accuracy without Window:  0.38953112658864075 %\n",
            "Test Accuracy with Window:  1.2141064212344008 %\n",
            "Accuracy without Window (Test + Training):  0.6655838874003361 %\n",
            "Accuracy with Window (Test + Training):  1.4698045057955706 %\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "matched = 0\n",
        "total = 0\n",
        "\n",
        "matched_with_window = 0\n",
        "total_with_window = 0\n",
        "inc = window_size-1\n",
        "for video_name, features_list, actual_labels in train_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        labels_for_video.extend(predicted_labels.tolist())\n",
        "        matched+=matches\n",
        "        total += total_labels\n",
        "        matched_with_window += matches\n",
        "        total_with_window +=total_labels\n",
        "    matched_with_window += inc\n",
        "    total_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Train Accuracy without Window: \", (matched * 100)/total, \"%\")\n",
        "print(\"Train Accuracy with Window: \", (matched_with_window * 100)/total_with_window, \"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "matched_test_with_window = 0\n",
        "total_test_with_window = 0\n",
        "for video_name, features_list, actual_labels in test_loader_:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        labels_for_video.extend(predicted_labels.tolist())\n",
        "        matched_test+=matches\n",
        "        total_test += total_labels\n",
        "        matched_test_with_window += matches\n",
        "        total_test_with_window += total_labels\n",
        "    matched_test_with_window += inc\n",
        "    total_test_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Test Accuracy without Window: \", (matched_test*100)/total_test, \"%\")\n",
        "print(\"Test Accuracy with Window: \", (matched_test_with_window*100)/total_test_with_window, \"%\")\n",
        "\n",
        "print(\"Accuracy without Window (Test + Training): \", ((matched_test+matched)*100)/(total_test+total), \"%\")\n",
        "print(\"Accuracy with Window (Test + Training): \", ((matched_test_with_window+matched_with_window)*100)/(total_test_with_window+total_with_window), \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mI2ycMbRqu-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Level 4: 80%\n",
        "Level 3: 86.11%\n",
        "Level 2:\n",
        "Level 1:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRWVac4p0tws"
      },
      "source": [
        "# Testing wiht only first (Window_Size - 1) true descriptors per video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjjXlAdnCb5I"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(Dataset):\n",
        "    def __init__(self, video_paths, frames_to_clusters_mapping, scaler=None, window_size=10):\n",
        "        \"\"\"\n",
        "        Dataset for generating sliding windows for an entire video.\n",
        "\n",
        "        Args:\n",
        "        - video_paths: List of paths to video CSVs.\n",
        "        - scaler: Scaler to normalize the features.\n",
        "        - window_size: Number of rows in each sliding window.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.scaler = scaler\n",
        "        self.window_size = window_size\n",
        "        self.frames_to_clusters_mapping = frames_to_clusters_mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        features, labels = extract_wav2vec_features_and_labels_from_csv(video_path)\n",
        "        video_name = video_path.replace(\".csv\",'').split(\"/\")[-1]\n",
        "        # Normalize features using the scaler\n",
        "        if self.scaler:\n",
        "            features = self.scaler.transform(features)\n",
        "\n",
        "        num_rows = labels.shape[0]\n",
        "        sliding_windows_features = []\n",
        "        sliding_windows_labels = []\n",
        "        actual_descriptors = labels[0:self.window_size-1]\n",
        "\n",
        "        #Getting ground truth for the first Window_Size - 1 frames\n",
        "        for i in range(self.window_size-1):\n",
        "          sliding_windows_labels.append(self.frames_to_clusters_mapping[video_name][i])\n",
        "        # Generate sequential windows\n",
        "        for start in range(num_rows - self.window_size + 1):\n",
        "            end = start + self.window_size\n",
        "            # Select the current sliding window features and corresponding labels\n",
        "            window_features = features[start:end]\n",
        "\n",
        "\n",
        "            # Concatenate the first (N-1) labels with the N features for input\n",
        "            # Shape: (N, 28) for features, and (N-1,) for labels\n",
        "            input_vector = window_features.flatten()\n",
        "            sliding_windows_features.append(input_vector)\n",
        "            # The Nth label is the target/output\n",
        "            target_label = self.frames_to_clusters_mapping[video_name][end-1]\n",
        "            sliding_windows_labels.append(target_label)\n",
        "\n",
        "        return {\n",
        "            \"video\": video_name,\n",
        "            \"features\": [torch.tensor(w, dtype=torch.float32) for w in sliding_windows_features],\n",
        "            \"labels\": [w for w in sliding_windows_labels],\n",
        "            \"descriptors\": actual_descriptors,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpX_a-MZtzMk"
      },
      "outputs": [],
      "source": [
        "def sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn(batch):\n",
        "    video_name = batch[0]['video']\n",
        "    features = batch[0][\"features\"]  # List of sliding windows for each video\n",
        "    labels = batch[0][\"labels\"]  # Corresponding labels\n",
        "    actual_descriptors = batch[0][\"descriptors\"]\n",
        "    return video_name, features, labels, actual_descriptors\n",
        "\n",
        "\n",
        "# Initialize the sliding window per video dataset\n",
        "train_dataset__ = SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(train_video_paths, frames_data, scaler=scaler, window_size=window_size)\n",
        "test_dataset__ = SlidingWindowwithWindowSizeTruDescriptorsPerVideoDatasetForAccuracy(test_video_paths,frames_data, scaler=scaler, window_size=window_size)\n",
        "\n",
        "# Initialize the data loaders\n",
        "train_loader__ = DataLoader(train_dataset__, batch_size=1, shuffle=False, collate_fn=sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn)\n",
        "test_loader__ = DataLoader(test_dataset__, batch_size=1, shuffle=False, collate_fn=sliding_window_withWindowSizeTruDescriptors_per_video_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY79NYZysDBR"
      },
      "outputs": [],
      "source": [
        "def updateTrueDescriptorsList(actual_descriptors, window_size, new_predicted_descriptor):\n",
        "    # Roll the array by 1 (shifts all rows by one, and the last row will be empty)\\\n",
        "    if window_size != 2:\n",
        "      actual_descriptors = np.roll(actual_descriptors, shift=-1, axis=0)\n",
        "\n",
        "      # Add the new predicted descriptor at the last row\n",
        "      actual_descriptors[window_size - 2] = new_predicted_descriptor.detach().cpu().numpy()\n",
        "    else:\n",
        "      actual_descriptors = new_predicted_descriptor.detach().cpu().numpy()\n",
        "    return actual_descriptors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting MSE Loss"
      ],
      "metadata": {
        "id": "eUC9V3wxmQoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5QK-xQjiNSl"
      },
      "outputs": [],
      "source": [
        "train_mse = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in train_loader__:\n",
        "    length = len(features_list)\n",
        "    #print(length)\n",
        "    predicted_descriptors = []\n",
        "    #print(true_descriptors[video_name].shape)\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        #print(outputs.shape)\n",
        "        predicted_descriptors.append(outputs.detach().cpu().numpy())\n",
        "    predicted_descriptors = np.vstack(predicted_descriptors)\n",
        "    predicted_descriptors = torch.tensor(predicted_descriptors, dtype=torch.float32).to(device)\n",
        "    actual_descriptors_ = true_descriptors[video_name][1:predicted_descriptors.shape[0]+1]\n",
        "    actual_descriptors_ = torch.tensor(actual_descriptors_, dtype=torch.float32).to(device)\n",
        "    mse_loss = F.mse_loss(predicted_descriptors, actual_descriptors_, reduction = 'mean')\n",
        "    train_mse += mse_loss.item()\n",
        "print(\"Average MSE Train:\", train_mse/len(train_loader__))\n",
        "\n",
        "\n",
        "test_mse = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in test_loader__:\n",
        "    length = len(features_list)\n",
        "    #print(length)\n",
        "    predicted_descriptors = []\n",
        "    #print(true_descriptors[video_name].shape)\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        #print(outputs.shape)\n",
        "        predicted_descriptors.append(outputs.detach().cpu().numpy())\n",
        "    predicted_descriptors = np.vstack(predicted_descriptors)\n",
        "    predicted_descriptors = torch.tensor(predicted_descriptors, dtype=torch.float32).to(device)\n",
        "    actual_descriptors_ = true_descriptors[video_name][1:predicted_descriptors.shape[0]+1]\n",
        "    actual_descriptors_ = torch.tensor(actual_descriptors_, dtype=torch.float32).to(device)\n",
        "    mse_loss = F.mse_loss(predicted_descriptors, actual_descriptors_, reduction = 'mean')\n",
        "    test_mse += mse_loss.item()\n",
        "print(\"Average MSE Test:\", test_mse/len(test_loader__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QbFjkLkSDZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with open(\"/content/drive/MyDrive/Sawaiz_2/results_pkl_videos/model_NextFramePredictor_4 videos_predicted_and_actual_frame_descriptors.pkl\", \"wb\") as file:\n",
        "#     pickle.dump(results, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCmbzRoUtDxf",
        "outputId": "9727df8e-d6fe-437e-fde5-882f28a2cb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy without Window:  1.4960854895426583 %\n",
            "Train Accuracy with Window:  2.2926591250529667 %\n",
            "Test Accuracy without Window:  1.2655273928340633 %\n",
            "Test Accuracy with Window:  2.0828511918537376 %\n",
            "Accuracy without Window (Test + Training):  1.4847911135889384 %\n",
            "Accuracy with Window (Test + Training):  2.2823793545280813 %\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "matched = 0\n",
        "total = 0\n",
        "\n",
        "matched_with_window = 0\n",
        "total_with_window = 0\n",
        "inc = window_size-1\n",
        "for video_name, features_list, actual_labels, actual_descriptors in train_loader__:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        labels_for_video.extend(predicted_labels)\n",
        "        matched+=matches\n",
        "        total += total_labels\n",
        "        matched_with_window += matches\n",
        "        total_with_window +=total_labels\n",
        "    matched_with_window += inc\n",
        "    total_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Train Accuracy without Window: \", (matched * 100)/total, \"%\")\n",
        "print(\"Train Accuracy with Window: \", (matched_with_window * 100)/total_with_window, \"%\")\n",
        "\n",
        "matched_test = 0\n",
        "total_test = 0\n",
        "matched_test_with_window = 0\n",
        "total_test_with_window = 0\n",
        "for video_name, features_list, actual_labels, actual_descriptors in test_loader__:\n",
        "    labels_for_video =[]\n",
        "    for i, features in enumerate(features_list):\n",
        "      features = np.concatenate((features, actual_descriptors.flatten()))\n",
        "      features = torch.tensor(features, dtype=torch.float32)\n",
        "      features = features.to(device)\n",
        "      features = features.unsqueeze(0)\n",
        "      if features.shape[1] == (window_size*28 + output_size*(window_size-1)):\n",
        "        outputs = model(features)  # Obtain model predictions directly in torch\n",
        "        predicted_labels, matches, total_labels = evaluate_video_predictions(outputs, clusters_descriptors, [actual_labels[i+window_size-1]], device, cluster_nearest_neighbors)\n",
        "        predicted_labels = predicted_labels.tolist()\n",
        "        outputs = torch.tensor(clusters_descriptors[predicted_labels[0]], dtype=torch.float32).to(device)\n",
        "        actual_descriptors = updateTrueDescriptorsList(actual_descriptors, window_size, outputs)\n",
        "        labels_for_video.extend(predicted_labels)\n",
        "        matched_test+=matches\n",
        "        total_test += total_labels\n",
        "        matched_test_with_window += matches\n",
        "        total_test_with_window += total_labels\n",
        "    matched_test_with_window += inc\n",
        "    total_test_with_window += inc\n",
        "    results[video_name] = labels_for_video\n",
        "\n",
        "print(\"Test Accuracy without Window: \", (matched_test*100)/total_test, \"%\")\n",
        "print(\"Test Accuracy with Window: \", (matched_test_with_window*100)/total_test_with_window, \"%\")\n",
        "\n",
        "print(\"Accuracy without Window (Test + Training): \", ((matched_test+matched)*100)/(total_test+total), \"%\")\n",
        "print(\"Accuracy with Window (Test + Training): \", ((matched_test_with_window+matched_with_window)*100)/(total_test_with_window+total_with_window), \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRVjvuyByIvO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Level 4: 40.13%\n",
        "Level 3: 55.6%\n",
        "Level 2:\n",
        "Level 1:\n",
        "'''\n",
        "# with open(\"/content/drive/MyDrive/Sawaiz_2/results_pkl_videos/model_NN_1stTrueFrame_NextFramePredictor_800_clusters.pkl\", \"wb\") as file:\n",
        "#     pickle.dump(results, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87gRFj-qXrHM",
        "outputId": "faa09cef-d999-43b8-fff4-4ed67cf87eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaler saved to /content/drive/MyDrive/Sawaiz_2/results_pkl_videos/scaler_MEAD2.pkl\n"
          ]
        }
      ],
      "source": [
        "# import joblib\n",
        "\n",
        "# # Save the trained scaler to a file\n",
        "# scaler_filename = \"/content/drive/MyDrive/Sawaiz_2/results_pkl_videos/scaler_MEAD2.pkl\"\n",
        "# joblib.dump(scaler, scaler_filename)\n",
        "# print(f\"Scaler saved to {scaler_filename}\")\n",
        "# # Load the scaler from the file\n",
        "# scaler = joblib.load(scaler_filename)\n",
        "# print(\"Scaler loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb7UVslJCdGv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNBR48iZJM2k2uvYGMmac3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}